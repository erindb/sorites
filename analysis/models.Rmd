---
title: "Sorites Model Fits"
author: "Erin Bennett"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tikz}
- \usetikzlibrary{bayesnet}
- \usepackage{amsmath}
- \usepackage[makeroom]{cancel}
---

```{r global_options, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = F, warning = F, cache = F, message = F,
                      sanitiz = F, fig.width = 5, fig.height = 3)
```

```{r load (and reformat) data}
source("utils.R")
source("reformatting_data.R")
df = load_sorites(F)
give_a_number = load_give_a_number(F)
prior_bins_data = load_priors(F)
```

# Model

## Definitions

* $w_i$ := width of histogram bins for item $i$
* $x_i$ := sample in give a number trial
* $P_{ib}$ := true probability of bin $b$ for item $i$
* $d_{ib}$ := slider rating for bin $b$ for item $i$
* $S2(I_{i\varepsilon})$ := RSA S2(L1(expensive) + $\varepsilon$) for item $i$
* $S2(C_{iv})$ := RSA S2(expensive) for item $i$
* $s^I_{i\varepsilon}$ := binarization of likert rating for inductive premise for item $i$ and epsilon $\varepsilon$
* $s^C_{iv}$ := binarization of likert rating for concrete premise for item $i$ and value $v$
* $\alpha^I_2$ := speaker rationality for S1 for inductive premise

## Diagram

\tikz{
  \node[latent,] (mu) {$\mu_i$};
  \node[latent, right=of mu] (sig) {$\sigma_i$};
  
  \node[above=of mu] (mumax) {$\mu^{max}$};
  \node[above=of sig] (sigmax) {$\sigma^{max}$};
  
  \node[latent, right=4cm of sigmax] (sigbin) {$\sigma_{bin}$};
  \node[right=of sigbin] (alpha1max) {$\alpha_1^{max}$};
  \node[right=of alpha1max] (alpha2max) {$\alpha_2^{max}$};
  \node[right=of alpha2max, xshift=2cm] (costmax) {$c_{max}$};
  
  \node[det, below=of mu, below=of sig] (pbin) {$P_{ib}$};
  \node[obs, below=of pbin] (dbin) {$d_{ib}$};
  
  \node[obs, left=of pbin] (giveanum) {$x_{i}$};
  
  \node[latent, below=of costmax] (cost) {$c$};
  \node[latent, below=of alpha1max] (alpha2C) {$\alpha_{2}^{C}$};
  \node[latent, below=of alpha2max] (alpha1I) {$\alpha_{1}^{I}$};
  \node[latent, left=of alpha2C] (alpha1C) {$\alpha_{1}^{C}$};
  \node[latent, right=of alpha1I] (alpha2I) {$\alpha_{2}^{I}$};
  
  \node[latent, below=of pbin, xshift=2cm] (s2concrete) {$S2(C_{iv})$};
  \node[latent, right=of s2concrete] (s2inductive) {$S2(I_{i\varepsilon})$};
  
  \node[obs, below=of s2inductive] (sI) {$s^{I}_{i\varepsilon}$};
  \node[obs, below=of s2concrete] (sC) {$s^{C}_{iv}$};
  
  \node[above=of sigbin] (sigbinmax) {$\sigma_{bin}^{max}$};
  
  \edge {mumax} {mu}
  \edge {sigmax} {sig}
  \edge {mu, sig} {pbin}
  \edge {mu, sig} {giveanum}
  \edge {pbin, sigbin} {dbin}
  \edge {pbin} {s2inductive, s2concrete}
  \edge {s2inductive} {sI}
  \edge {s2concrete} {sC}
  \edge {alpha1I, alpha2I} {s2inductive}
  \edge {alpha1C, alpha2C} {s2concrete}
  \edge {alpha1max} {alpha1C, alpha1I}
  \edge {alpha2max} {alpha2C, alpha2I}
  \edge {cost} {s2concrete, s2inductive}
  \edge {costmax} {cost}
  \edge {sigbinmax} {sigbin}
  
  \plate {bin} {
    (pbin) (dbin)
  } {$b\in{Bins_i}$};
  \plate {epsilon} {
    (s2inductive) (sI)
  } {$\varepsilon \in Epsilons$};
  \plate {value} {
    (s2concrete) (sC)
  } {$v \in Values$};
  \plate {item} {
    (mu) (sig)
    (giveanum)
    (bin.south east)
    (epsilon.south east)
    (value.south east)
    (s2inductive) (s2concrete)
  } {$i\in{Objects}$};
}

## Distributions/Functions/Values:

Experiment design parameters:

* $Objects$
* $Bins$
* $Epsilons$
* $Values$

Assumed model parameters:

* $\mu^{max}$ = 20
* $\sigma^{max}$ = 5
* $\sigma_{binned\ hist}$ = ??
* $\alpha_1^{max}$ = 20
* $\alpha_2^{max}$ = 5
* $\sigma_{bin}^{max}$ = 5

Inferred Latent variables:

* $\mu_i \sim \mathcal{U}\{0, \mu^{max}\}$
* $\sigma_i \sim \mathcal{U}\{0, \sigma^{max}\}$
* $\alpha_1^I \sim \mathcal{U}\{0, \alpha_1^{max}\}$
* $\alpha_2^I \sim \mathcal{U}\{0, \alpha_2^{max}\}$
* $\alpha_1^C \sim \mathcal{U}\{0, \alpha_1^{max}\}$
* $\alpha_2^C \sim \mathcal{U}\{0, \alpha_2^{max}\}$
* $\sigma_{bin} \sim \mathcal{U}\{0, \sigma_{bin}^{max}\}$
* $P_{ib} = \int_{LB_{ib}}^{UB_{ib}} \varphi(\ln(t)|\mu_i, \sigma_i) dt$

Observations from experimental data:

* $logit(d_{ib}) \sim \mathcal{N}(logit(p_{ib}), \sigma_{bin})$
* $\ln(x_i) \sim \mathcal{N}(\mu_i, \sigma_i)$
* $s_{iv}^C$
* $s_{i\varepsilon}^I$

# Model fit

## Give a Number ~ Log Normal

\tikz{
  \node[latent,] (mu) {$\mu_i$};
  \node[latent, right=of mu] (sig) {$\sigma_i$};
  
  \node[above=of mu] (mumax) {$\mu^{max}$};
  \node[above=of sig] (sigmax) {$\sigma^{max}$};
  
  \node[obs] (giveanum) {$x_{i}$};
  
  \edge {mumax} {mu}
  \edge {sigmax} {sig}
  \edge {mu, sig} {giveanum}
  
  \plate {item} {
    (mu) (sig)
    (giveanum)
  } {$i\in{Objects}$};
}

Model fit for Give a Number as log normal using Incremental MH.

```{r, echo = T}
iterations = 5000
burn = iterations/2
lag = 1
```

```{r}
RUN_MODEL = F
if (RUN_MODEL) {
  raw_rs = webppl(
      program_file = model_dir("bda_rsa_priors.wppl"),
      inference_opts = list(method="incrementalMH",
                            samples=iterations,
                            burn=burn,
                            verbose = T,
                            verboseLag = iterations / 10,
                            lag=lag),
      model_var = "give_a_number_model",
      data_var = "ARGS",
        packages = c(model_dir("/node_modules/utils/")),
        data = list(priorbins = prior_bins_data,
                    sorites = select(df, qtype, dollar_amount, object, response),
                    give_a_number = give_a_number,
                    nbins = 10)
    )
    rs = cbind(raw_rs, do.call(rbind,lapply(char(raw_rs$Parameter),function(jsonkey) {return(fromJSON(jsonkey) %>% as.data.frame)}))) %>%
      select(-Parameter) %>%
      mutate(dollar_amount = num(dollar_amount))
    write.csv(rs,
              cache_dir(paste("give_a_number_model_cache", iterations, ".csv", sep="")),
              row.names = F)
} else {
  rs = read.csv(cache_dir(paste("give_a_number_model_cache", iterations, ".csv", sep="")))
}
give_a_number_fit = rs
```

```{r, fig.width=10, fig.height=2}
give_a_number_fit %>% filter(type=="prior" & (label=="mu" | label=="sigma")) %>%
 # group_by(Iteration, label, object) %>%
  #summarise(value = mean(value)) %>%
  spread(label, value) %>%
  rowwise() %>%
  mutate(log_price = rnorm(1, mean = mu, sd = sigma),
         price = exp(log_price),
         src = "model") %>%
  as.data.frame %>%
  select(price, object, src) %>%
  rbind(., (give_a_number %>% mutate(src="data"))) %>%
  as.data.frame %>%
  ggplot(., aes(x=price, color=src)) +
  stat_ecdf(geom="step") +
  facet_wrap(~object, scales="free", ncol=5)+
  xlim(0, 1000)
ggsave("images/give_a_number_fit.png", width=10, height=2)
```

```{r, fig.width=10, fig.height=4}
give_a_number_fit %>% filter(type=="prior" & (label=="mu" | label=="sigma")) %>%
 # group_by(Iteration, label, object) %>%
  #summarise(value = mean(value)) %>%
  spread(label, value) %>%
  rowwise() %>%
  mutate(log_price = rnorm(1, mean = mu, sd = sigma),
         price = exp(log_price),
         src = "model") %>%
  as.data.frame %>%
  select(price, object, src) %>%
  rbind(., (give_a_number %>% mutate(src="data"))) %>%
  as.data.frame %>%
  ggplot(., aes(x=price, fill=src)) +
  geom_histogram(bins=30)+
  facet_wrap(src~object, scales="free", ncol=5)+
  xlim(0, 1000)
ggsave("images/give_a_number_fit2.png", width=10, height=4)
```

```{r, fig.width=7, fig.height=3}
give_a_number_fit %>% filter(type=="prior" & (label=="mu" | label=="sigma")) %>%
  ggplot(., aes(x=value)) +
  geom_histogram() +
  facet_grid(label~object, scales="free")
```

## Concrete Premise ~ lifted L1

For a lifted $L_1$ model, we need to some nested inference. But there's some simplification we can do.

$$
\begin{aligned}
L_0(x \ | \ u, \theta) \ &\propto \ P(x) \cdot \delta_{x, u, \theta}
  \\
  &L_0(x \ | \ \text{expensive}, \theta) = 
    \begin{cases}
     \frac{1}{\int_\theta^\infty P(x)dx} \cdot P(x) & \text{if } x>\theta \\
     0 & \text{otherwise}
    \end{cases}
  \\ \\
  &L_0(x \ | \ \text{no utterance}, \theta) = P(x)
  \\ \\
S_1(u \ | \ x, \theta) &\propto P(u) \cdot \left(L_0(x \ | \ u, \theta)\right)^{\alpha_1}
  \\ \\
  &S_1(\text{expensive} \ | \ x > \theta) \propto P(\text{expensive}) \cdot \left(\frac{1}{\int_\theta^\infty P(x)dx}\right)^{\alpha_1} \cancel{\left(P(x)\right)^{\alpha_1}} \\
  &S_1(\text{no utterance} \ | \ x > \theta) \propto P(\text{no utterance}) \cdot \cancel{\left(P(x)\right)^{\alpha_1}}
  \\ \\
  &S_1(\text{expensive} \ | \ x \leq \theta) = 0 \\
  &S_1(\text{no utterance} \ | \ x \leq \theta) = 1
\end{aligned}
$$

So the $S_1$ score for expensive is:

$$
\begin{aligned}
\log\left(
  P(\text{expensive}) \cdot
  \left( \frac{1}{\int_\theta^\infty P(x)dx} \right)^{\alpha_1}
\right) &= -cost(\text{expensive}) +
\log\left(
  \left( \frac{1}{\int_\theta^\infty P(x)dx} \right)^{\alpha_1}
\right)\\
&= -cost(\text{expensive}) -\alpha_1
\log\left(\int_\theta^\infty P(x)dx
\right)\\
&= -cost(\text{expensive}) -\alpha_1
\log\left( P(x > \theta) \right)
\end{aligned}
$$


<!--
$$
  
  
S_1(u \ | \ x, \theta) \propto P(u) \cdot &L_0(x \ | \ u, \theta)
  &L_0(x \ | \ \text{no utterance}, \theta) = P(x) \\ & \\
  
S_1(u | x, \theta) \propto &P(u) \cdot L_0(x | u, \theta) \\ &

S_1(u | x, \theta) \propto &P(u) \cdot L_0(x | u, \theta) \\ \\

&S_1(\text{``expensive''} | x > \theta) \propto 
  P(\text{``expensive''})\frac{1}{\int_\theta^\infty P(x)dx} P(x) \\
&S_1(\text{\emph{no utterance}} | x > \theta) \propto
  P(\text{\emph{no utterance}})P(x) \\ \\

&S_1(\text{``expensive''} | x \leq \theta) \propto 0 \\
&S_1(\text{\emph{no utterance}} | x \leq \theta) \propto P(x) \\
\end{aligned}
$$
-->

<!--
I'm pretty sure I need to bin in order for this to be tractable.

Here's the median prior distribution for laptops:

```{r}
params = give_a_number_fit %>%
  filter(type == "prior" &
           (label == "mu" | label=="sigma") &
           object == "laptop") %>%
  group_by(object, label) %>%
  summarise(value = median(value)) %>%
  named_vec(label, value)
price_cdf = function(x) {
  return(plnorm(x, params["mu"], params["sigma"]))
}
max_value = 1000
bin_width = 200
x = seq(0, max_value, 1)
b = seq(bin_width/2, max_value, bin_width)
upper = b + (bin_width/2)
lower = b - (bin_width/2)
px = dlnorm(x, params["mu"], params["sigma"])
pb = price_cdf(upper) - price_cdf(lower)
pb[length(pb)] = 1 - price_cdf(lower[length(lower)])
cdf_max = max(pb)
df_max = max(px)
px = cdf_max/df_max * px
ggplot() +
  geom_bar(aes(x=b, y=pb), stat = "identity", fill = "gray") +
  geom_line(aes(x=x, y=px))
```


What the speaker actually cares about is $L(x | u)$, the listener's probability density of choosing price $x$ given utterance $u$. But if I bin, what I *have* is $L(b | u)$, the probability that the listener will choose the bin $b$ containing $x$.

$L_0(x | u, \theta) \propto Pr(x) \cdot \delta_{\text{truth value}(u | x, \theta)}$

So for no utterance:
$L_0(x | \text{no utterance}, \theta) = Pr(x)$

And for "expensive":
$L_0(x | \text{expensive}, \theta) = \left\{ \frac{1}{M} Pr(x) \right\}$

And then:

$S_1(u | x, \theta) \propto Pr(u) \cdot L_0(x | u, \theta)$

so:

$S_1(\text{no utterance} | x, \theta) \propto Pr(\text{no utterance}) \cdot Pr(x)$

and

$S_1(\text{expensive} | x, \theta) \propto Pr(\text{expensive}) \cdot \frac{1}{M} Pr(x)$

$L_0$ always just thresholds and re-scales based on the probability mass above the threshold. For the continuous version, this probability mass (call it $M$) is the exact probability mass above the threshold, so $S1(\text{expensive} | x, \theta) \propto Pr(u)Pr(x)$. For the binned version, 


This isn't really affected by different binning methods. That is $P(b | u)$ is scaled by $M$ and $P(x | u)$ is also scaled by $M$ (or at least, an approcimation of $M$).

If the bins are all equal widths, I think this is fine. Why do I think this is fine?

Let's say $\theta=500$. Then when the utterance is null, the L0 dist is:

```{r, fig.width=3, fig.height=2}
price_cdf = function(x) {
  return(plnorm(x, params["mu"], params["sigma"]))
}
max_value = 1000
bin_width = 200
x = seq(0, max_value, 1)
cutoffs = c(0, 400, 450, 500, 700, 750, 900, 1000)
upper = cutoffs[2:length(cutoffs)]
lower = cutoffs[1:(length(cutoffs)-1)]
b = (lower + upper) / 2
px = dlnorm(x, params["mu"], params["sigma"])
pb = price_cdf(upper) - price_cdf(lower)
pb[length(pb)] = 1 - price_cdf(lower[length(lower)])
cdf_max = max(pb)
df_max = max(px)
px = cdf_max/df_max * px
lookup_bins = named_vec(NULL, upper, pb)
xb = c(matrix(c(lower, b, upper), ncol = 3, byrow = F) %>% t)
pbins = rep(pb, each=3)
ggplot() +
  geom_line(aes(x=xb, y=pbins), colour = "gray") +
  geom_line(aes(x=x, y=px)) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

And if the utterance is "expensive":

```{r, fig.width=3, fig.height=2}
pbins = pbins[xb >= 500]
xb = xb[xb >= 500]
px = px[x >= 500]
x = x[x >= 500]
ggplot() +
  geom_line(aes(x=xb, y=pbins), colour = "gray") +
  geom_line(aes(x=x, y=px))+
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```


As an estimate, I could just scale this by the width of the bin.


$$P(x | b) \approx $$
-->


```{r params, echo = T}
# iterations = 10#5000
# burn = iterations/2
# lag = 1
```

```{r run.concrete.model}
# RUN_MODEL = T
# if (RUN_MODEL) {
#   start_time = Sys.time()
#   raw_rs = webppl(
#     program_file = "models/bda_rsa_priors.wppl",
#     inference_opts = list(method="incrementalMH",
#                           samples=iterations,
#                           burn=burn,
#                           verbose = T,
#                           verboseLag = iterations / 10,
#                           lag=lag),
#     model_var = "concrete_premise_model",
#     data_var = "ARGS",
#     packages = c("models/node_modules/utils/"),
#     data = list(priorbins = prior_bins_data,
#                 sorites = select(df, qtype, dollar_amount, object, response),
#                 give_a_number = give_a_number,
#                 upper_bounds = upper_bounds,
#                 internal_iterations = 100)
#   )
#   end_time = Sys.time()
#   print(end_time - start_time)
#   rs = cbind(raw_rs, do.call(rbind,lapply(char(raw_rs$Parameter),function(jsonkey) {return(fromJSON(jsonkey) %>% as.data.frame)}))) %>%
#     select(-Parameter) %>%
#     mutate(dollar_amount = num(dollar_amount))
#   write.csv(rs,
#             paste(".concrete_model_cache", iterations, ".csv", sep=""),
#             row.names = F)
# } else {
#   rs = read.csv(paste(".concrete_model_cache", iterations, ".csv", sep=""))
# }
# concrete_premise_fit = rs %>% mutate(
#   UB = num(UB),
#   LB = num(LB),
#   mid = num(mid)
# )
```

```{r}
# RUN_CONCRETE_AGG = T
# if (RUN_CONCRETE_AGG) {
#   concrete_model_agg = concrete_premise_fit %>% filter(type=="concrete") %>%
#     group_by(object, dollar_amount) %>%
#     summarise(low = ci.low(value),
#               high = ci.high(value),
#               value = mean(value))
#   write.csv(concrete_model_agg,
#             paste(".concrete_model_agg_cache", iterations, ".csv", sep=""),
#             row.names = F)
# } else {
#   concrete_model_agg = read.csv(paste(".concrete_model_agg_cache", iterations, ".csv", sep=""))
# }
# concrete_model_agg %>%
#   ggplot(., aes(x=dollar_amount, y=value)) +
#   geom_point(alpha=opacity) +
#   geom_errorbar(aes(ymin=low, ymax=high), width=0, alpha=opacity) +
#   facet_grid(~object, scales="free")
# # unique((agg %>% filter(qtype=="concrete" & object=="laptop"))$dollar_amount) %>% sort
```

```{r, fig.width=12, fig.height=3}
# df %>% filter(qtype=="concrete") %>% select(object, dollar_amount, response) %>%
#   mutate(src = "data") %>%
#   rbind(., concrete_premise_fit %>% filter(type=="concrete") %>%
#           mutate(response = value, src = "model") %>%
#           select(object, dollar_amount, response, src)) %>%
#   as.data.frame %>%
#   ggplot(., aes(x=dollar_amount, color=src)) +
#   stat_ecdf(geom="step") +
#   facet_wrap(~object, scales="free", ncol=5)
```


```{r, fig.width=10, fig.height=2}
# concrete_premise_fit %>% filter(type=="prior" & (label=="mu" | label=="sigma")) %>%
#  # group_by(Iteration, label, object) %>%
#   #summarise(value = mean(value)) %>%
#   spread(label, value) %>%
#   rowwise() %>%
#   mutate(log_price = rnorm(1, mean = mu, sd = sigma),
#          price = exp(log_price),
#          src = "model") %>%
#   as.data.frame %>%
#   select(price, object, src) %>%
#   rbind(., (give_a_number %>% mutate(src="data"))) %>%
#   as.data.frame %>%
#   ggplot(., aes(x=price, color=src)) +
#   stat_ecdf(geom="step") +
#   facet_wrap(~object, scales="free", ncol=5)+
#   xlim(0, 1000)
```

<!-- ### Questions

1. concrete -> inductive and inductive -> concrete
    - if you don't have a constrained way to get from priors to posteriors, then you will overfit (bayes factor)
2. priors -> threshold distributions
    - 

* If we lesion data from the model can we infer it?
* Is there a regression to compare to?
* Can we argue that RSA is necessary?
* What other thresholds could be used?
    - quantiles
* priors + rsa arrrrrrrow posteriors to fit data
* stone model not from priors -->
