---
title: "Sorites"
author: ""
header-includes:
   - \usepackage{float}
output:
  pdf_document:
    number_sections: true
    fig_caption: true
bibliography: sorites.bib
---

```{r global_options, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = F, warning = F, cache = F, message = F,
                      sanitiz = F, fig.width = 5, fig.height = 3)
```

```{r}
# this is due friday
# 4 hours of work
# bullet point of architecture.
# one sentence for each bullet point.
# and a couple sub bullet points for each paragraphs.
# 4-5 paragraphs for introductions
```

# Introduction

```{r libraries}
library(knitr)
source("../analysis/startup.R")
library(ggrepel)
set.seed(123)
library(corrplot)
```

* sorites paradox exists
    * sorites paradox is ...
    * it's confusing with a binary, logical semantics

* dan's adjective model shows the qualitative behavior
    * model the premises as speech acts, which depend on context, rather than logical statements that are universally true or false
    * we get a _probability_ of endorsement, which gives us graded endorsements
    * depending on the prior distribution over values, we might get different endorsements for the same value (e.g. different objects have difference price distributions, so inductive for coffee maker as some dollar amount epsilong will get different endorsement than the same statement about laptops)

* do humans actually show the behavior predicted by this model?
    * we collect endorsements for sorites-style utterancs for different prices for a range of everyday objects
    * we also elicit prior distributions over prices for each of thos object categories.
    * finally, we translate the sorites premises into speech acts where a speaker has a goal of communicating some information to the listener

* the endorsement expt shows the qualitative pattern of the sorites paradox, and also we can quantitatively predict the endorsements given the separate experiment eliciting the price distributions.
    
    
```{r}
# one property of vagueness is sorites.
# statements which are true on their own, but when taken together logically imply something false.
# because nothing is all true in all contexts but we average across contexts to get an estimate of the value.
# and if it's true in enough situations then we consider it basically true
# but there's still a few situations where that's not he case and as we increase the distance between the hypothetical items in the inductive premise we see less and less endorsement
# it's a paradox because if language were more binary true-false, if this was always true, then it would be a contradiction.
# dan wrote a lot about sorites and how that relates to vagueness
# intensifiers, sorites, and causal verbs
# intensifiers show how form can influence interpretations
# sorites shows how 
# sorites paradox intensifiers causal verbs
# dan talked a lot about this and we show empirically that this matches up with how people actually respond to the sorites premises
# in particular we chose a set of premises where the paradox holds but then we also show other situations where ... endorsement for these premises goes down as the specificity of the claim goes up.
# intensifiers, the more specific the claim, the more complex the form of the language
# sorites, the more specific the claim, the less endorsement. and this is a gradual drop off
# and so we can see that the sorites paradox happens at the specific point where endorsements are strong, so both claims... but we can continuously vary this parameter and get different responses
# but what about task demands
# yeah, that could be the reason we're getting the graded judgements. however the shape of the graded udgements is predicted by the price priors data so it's actually not... i thnk we're actually modeling the phenomenon because we're able to take one kind of information, feed it through this model with basically no parameters adn get out the data from this other experiment
# so we have this common model of the world that includes the prices of the objects and the way that people communicate with each other and we're asking different questions about that model
# and that's a good property of a model that you can ask different questions and get the answer
# so there's this specificity of the claim thing
# and the less specific the claim is, the easier it is to endorse
# so in particular the inductive premise has this specificity of the claim
# it would be nice to visualize across the different parameter values how much information is being conveyed, like what's the informativity of the utterances...
# sorites paradox exists
# here's some informationa bout it
# heree's somet hing people ahve siad about it thanks dan
# dan figured it out that basically this is a particular region -- like if there's two parameters, this price and this price, the the sorites premise gets endorsed in a particular way that creates a property in this reason so they lokok like theyre 100% certain but you can see as you vary the parm that these are continuous graded cjudgements so averaging over the contexts 
#  but then in this particular reason there's paradox I think) And he's in like has logical cement what we do which parametrizes language we use the vagueness and language in order to communicate
#   and as a result we're incorporating context Sedan has does not always parameterize the premises and you can see that at some perimeter sayings you get strong endorsement and at other settings you dont and this gradually drops off and this is a prediction of a model of sorites premises as speech acts, rather than an utterance with a truth value in and of itself, we think of it as a speech act, and since it's a speech act, we can take some context and derive some endorsement. and in this case it matches up with what people are doing. to get the context, we run this experiment. to get the endorsements we run this experiment, and then we take the context and predict the endrosement and voila the endorsements are predicted by the contexts with this model, quantitatively
#   so that's what people are doing
#   and you could do a linear regression on the log price, but then you'd have a random thingy for each object and each kind of question but you don't ahve to do that.
# that has a lot of parameters and this one really doesn't. there's also not a clear way to translate, there's not an explanation of how you get from the context to this information
# maybe it's derivable, but at that point you're saying the price distributions predict the endorsements
# sorites exists
# it was confusing
# but think of these logical statements as speech acts and it becomes less confusing
# and you can see how the variation of prices in the real world (the context we're supposing these speech acts occur within) -- those price distributions, the sorites endorsements fall out of the price distributions. we can derive the qualitative behavior of sorites paradox from these assumptions
# ok that's cool but does that actually match up with what people are doing?
# well we asked people both what the prices of objects were to elicit the context and what theirendorsements for sorites premise wer and we show that we can basically derive one from the other with no parameters and that's pretty cool.
# we show all of the qualitative behavior that dan predicted and we also show that this model explains the data
# in order to do that, we have to figure out what the speech act for these premises should be, but we did that
# the qualitative behavior of how they respod to premises and price distributions is kinda meh because task demands but taken together, it's pretty cool.
# the mapping between them is cool.
# there's task demands within each study, but there's no task demands across studies. there's no task demand reason why eliciting prices from one experiment should quantitatively predict the endorsements in the other experiment.
# so that's comforting.
# i should run different chains of the model to get errorbars.
# in the discussion, task demands. they're weird, but we're fine because there's no task demands across the two experiments, that's a between Ss design. that's cool
# task demands...
# then there's the parameters. you can predict these data with a linear regression but it would have a lot of parameters and you wouldn't be able to go from one thingamajig to the other i think
# 
```

# Experiment 1: Sorites Statements

```{r sorites data}
df11 = read.csv("../data/sorites/data_exp11_2015_06_05_14.csv", stringsAsFactors = F)
reward = df11$reward[[1]]
mean_time = df11$Answer.time_in_minutes %>% mean() %>% round()
df11 = df11 %>%
  select(workerid, Answer.phrasing, Answer.responses,
         Answer.time_in_minutes, Answer.subj_data) %>%
  rename(phrasing = Answer.phrasing,
         time = Answer.time_in_minutes)
df11 = do.call(rbind, mapply(
  function(w, subj_data, responses) {
    df = responses %>% fromJSON() %>% as.data.frame()
    df$workerid = w
    df$trial_number = 1:nrow(df)
    s = fromJSON(subj_data)
    df$language = s$language
    return(df)
  },
  df11$workerid,
  df11$Answer.subj_data,
  df11$Answer.responses,
  SIMPLIFY = F
))
df11$response = num(df11$response)
n_participants = df11 %>% group_by(qtype, object, dollar_amount) %>%
  summarise(n = length(response)) %>% .$n
stopifnot(min(n_participants) == max(n_participants))
n_participants_total = n_participants[[1]]
n_questions = df11 %>% group_by(workerid) %>%
  summarise(n = length(response)) %>% .$n
stopifnot(min(n_questions) == max(n_questions))
n_questions = n_questions[[1]]
likert_lower = min(df11$response)
likert_upper = max(df11$response)
stopifnot(length(likert_lower:likert_upper)==9)
df11 = df11 %>%
  filter(language %in% c("english", "English", "Engllish")) %>%
  mutate(response = response + 1)
n_participants = df11$workerid %>% unique() %>% length()
n_excluded = n_participants_total - n_participants
df11 %>% select(workerid, qtype, dollar_amount, object, response) %>%
  mutate(id = "11") %>%
  write.csv(file = "../data/sorites/final_sorites.csv")
```

## Participants

We recruited `r n_participants_total` participants with US IP addresses over Amazon's Mechanical Turk.
`r n_excluded` participants were exluded from analysis for not being native English speakers, leaving `r n_participants` participants for analysis.
The experiment took about `r mean_time` minutes and participants were paid `r reward`.

## Materials

The experiment consisted of `r n_questions` questions. There were 2 basic question types, *concrete* and *inductive* of the form:

* *Concrete*: An [OBJECT] that costs $[PRICE VALUE] is expensive.
* *Inductive*: An [OBJECT] that costs $[PRICE VALUE] is expensive.

There were 5 object categories (coffee maker, laptop, headphones, watch, and sweater) and 5 price values for each object category and premise type. The price values based on pilot experiments to constitute similar standard deviations of the price distribution for each object category and to capture a range of plausible values.

```{r sorites price tables}
dollar_amounts = df11 %>%
  group_by(qtype, object, dollar_amount, level) %>%
  summarise() %>%
  ungroup() %>%
  mutate(dollar_amount = sprintf("%.02f", dollar_amount)) %>%
  spread(object, dollar_amount) %>%
  select(-level)
dollar_amounts %>% filter(qtype=="concrete") %>% kable(caption="Price values for concrete premise.", align="rrrrrr")
dollar_amounts %>% filter(qtype=="inductive") %>% kable(caption="Price values for inductive premise.", align="rrrrrr")
```


## Methods

At the start of the experiment, participants were told they would be asked questions about the prices of different household items. Each question started with a statment in bold, either the *concrete* statement or the *inductive* statement.
For both types of questions, participants gave Likert responses for how much they agreed with the statement, on a scale from "Completely disagree" (1) to "Completely agree" (9).

Each participant then saw all 5 object categories with all 5 price values for each kind of sentence. Trials were presented in random order.

## Results

```{r sorites figure, fig.width=10, fig.height=5, fig.cap="Results of Experiment 1. Error bars are 95% confidence intervals."}
 breaks_fun <- function(x) {
  if (max(x) < 400) {
    c(0, 50, 100, 150, 200)
  # } else if (max(x) < 400) {
  #   c(0, 100, 200, 300)
  } else {
    c(0, 500, 1000, 1500, 2000)
  }
}
df11 %>%
  ggplot(aes(x=dollar_amount, y=response, colour=qtype, shape=qtype, linetype=qtype)) +
  # geom_point(alpha=0.1) +
  # geom_line(alpha=0.05, aes(group=workerid)) +
  stat_summary(geom="pointrange", fun.data="mean_cl_boot", alpha=0.7) +
  facet_wrap(qtype~object, ncol=5, scales="free_x") +
  scale_y_continuous(breaks=1:9, limits=c(0.9, 9.1)) +
  scale_x_continuous(breaks=breaks_fun) +
  xlab("Dollar Amount") +
  ylab("Endorsement Rating") +
  scale_colour_manual(values = c("darkgray", "black"))
```

```{r per item plots}
# df11 %>%
#   ggplot(aes(x=response)) +
#   facet_wrap(dollar_amount ~ qtype) +
#   geom_histogram(binwidth=1)
```

```{r per P plots}
# df11 %>%
#   ggplot(aes(x=response)) +
#   facet_wrap(~workerid) +
#   geom_histogram(binwidth=1)
```

Results of Experiment 1 are shown in Figure 1. We see a range of endorsements from very low (e.g. "A coffee maker that costs \$24.00 is expensive" or "A laptop that costs \$1850.00 less than an expensive laptop is expensive.") to very high (e.g. "A watch that costs \$2000.00 is expensive" or "A sweater that costs \$171.00 less than an expensive sweater is expensive."). We also see a gradual increase in endorsements for the concrete premises (the item is expensive) as the price of the item increases, and a gradual decrease in endorsements for the inductive premise (the less expensive item is expensive) as the _change_ in price between the expensive and less expensive items increases.

```{r cors between Ps}
get_cors = function(df) {
  sorites_cors = df %>% select(workerid, response, qtype, object, dollar_amount) %>%
    spread(workerid, response) %>%
    select(-c(qtype, object, dollar_amount)) %>%
    cor()
  # sorites_cors %>%
  #   corrplot()
  sorites_cors_ci = sorites_cors %>% .[upper.tri(.,diag = F)] %>% mean_cl_boot()
  return(sorites_cors_ci)
}
sorites_cors_ci = get_cors(df11) %>% round(3)
sorites_cors_inductive = get_cors(df11 %>% filter(qtype=="inductive")) %>% round(3)
sorites_cors_concrete = get_cors(df11 %>% filter(qtype=="concrete")) %>% round(3)

report_mean_ci = function(x) {
  paste("mean: ", x$y %>% round(3), ", CI: [", x$ymin %>% round(3), ", ", x$ymax %>% round(3), "]", sep="")
}
```

```{r cors by obj}
# df11 %>% group_by(object) %>%
#   do(get_cors(.)) %>%
#   ungroup()
# df11 %>% group_by(object) %>%
#   do(get_cors(.)) %>%
#   ungroup() %>%
#   ggplot(aes(x=object, y=y, ymin=ymin, ymax=ymax)) +
#   geom_pointrange()
# df11 %>% group_by(object, qtype) %>%
#   do(get_cors(.)) %>%
#   ungroup() %>%
#   ggplot(aes(x=object, y=y, ymin=ymin, ymax=ymax)) +
#   facet_grid(~qtype) +
#   geom_pointrange()
```

Participants tended to be in agreement about these endorsements. The average correlation between participants's responses was `r sorites_cors_ci$y %>% round(3)` (CI: [`r sorites_cors_ci$ymin %>% round(3)`, `r sorites_cors_ci$ymax %>% round(3)`]). 
Aggreement tended to be higher for the concrete premise (`r report_mean_ci(sorites_cors_concrete)`) than for the inductive premise (`r report_mean_ci(sorites_cors_inductive)`).

## Discussion

In this first experiment, we qualitatively capture the three characteristic effects of vagueness:

* Judgements depend on context: Different object categories and price values yield different responses.
* Judgements are systematic: Within a particular context, different people give similar judgements.
* Judgements are graded: They range from very low to very high endorsement and include borderline cases.

In particular, we see strong endorsement of some inductive premises. Importantly this is also graded, with some inductive sentences judged as not good at all.

This suggests rich quantitative patterns to explain.

# Experiment 2: Prior Distributions

Judgements to our sorites premises depended on the object categories. So, in order to model participants' endorsements, it was useful to have as input a measure of people's prior expectations about the prices of these object categories. In experiment 2, we elicit price distributions using a binned histogram approach [@franke_what_2016].

```{r priors data}
df12 = read.csv("../data/priors/data_exp12_2015_04_08.csv", stringsAsFactors = F)
reward = df12$reward[[1]]
df12 = df12 %>%
  select(workerid, starts_with("Answer"))
n_participants_total = nrow(df12)
mean_time = (num(df12$Answer.duration) %>% mean() / 60000) %>% round()
df12 = df12 %>%
  select(-c(Answer.duration, Answer.age, Answer.comments, Answer.cond)) %>%
  gather("trial", "data", -c(Answer.language, workerid))
df12 = df12 %>%
  ##actually, there's no need to leave out non-native speakers for this task
  # filter(Answer.language %in% c('"English"', '"ENGLISH"', '"english"')) %>%
  select(-Answer.language) %>%
  with(., do.call(rbind, mapply(function(w, t, d) {
    d = fromJSON(d)
    d = data.frame(lower = num(d$lowers),
               upper = d$uppers,
               response = num(d$responses),
               item = char(d$item),
               max = d$max,
               workerid = w,
               trial = t)
    return(d)
  }, workerid, trial, data, SIMPLIFY = F)))
df12 = df12 %>%
  mutate(upper = num(ifelse(upper=="infty", max, char(upper)))) %>%
  mutate(mid = (lower + upper)/2)
n_participants = df12$workerid %>% unique() %>% length()
n_excluded = n_participants_total - n_participants
df12 = df12 %>%
  group_by(workerid, item) %>%
  mutate(normed_response = response / sum(response)) %>%
  ungroup()
reformatted_df12 = df12 %>%
  rename(object = item, UB = upper, LB = lower, rating = response, normed_rating = normed_response) %>%
  group_by(workerid, object) %>%
  mutate(bin = 1:length(workerid)) %>%
  select(workerid, object, bin, UB, LB, rating, normed_rating)
reformatted_df12 %>%
  write.csv("../data/priors/final_priors.csv", row.names = F)
```

## Participants

We recruited `r n_participants_total` participants with US IP addresses over Amazon's Mechanical Turk.
The experiment took about `r mean_time` minutes and participants were paid `r reward`.

## Materials

![Screenshot from Experiment 2](img/priors_screenshot.png)

The experiment consisted of 5 question pages, on for each object category. Each page contained vertical slider bars corresponding to a range of prices (e.g. \$0 - \$50 or \$450-\$500). There were 50-80 sliders per page, depending on the object category (see Figure 2). The sliders were shown in rows of 10 sliders each.

There were 5 object categories (coffee maker, laptop, headphones, watch, and sweater). The price ranges for each object category were chosen based on pilot experiments. We wanted sufficient detail about the tails of the distributions, so we chose maximum values for each object category such that the average endorsement of the highest bin was very low. We also wanted sufficient granularity to address the sorites inductive premise, even for very small price values. We therefore chose the width of the bins so that, for every concrete price value \$$x$ and for every inductive price value \$$\varepsilon$ in our sorites premises experiment, we could confidently estimate the probability of an item \$$\epsilon$ less expensive than \$$x$. Our choise of maximum price and bin widths are shown in Table 3. The resulting distributions are fairly smooth, allowing us to interpolate within the bins as needed. Our level of resolution also allowed us to capture detail in especially dense parts of the distributions (usually the smaller ranges).

```{r priors price tables}
priors_table = data.frame(
  c("watch", "laptop", "coffee maker", "sweater", "headphones"),
  c(3000, 2500, 270, 240, 330),
  c(50, 50, 4, 6, 3))
names(priors_table) = c("object category", "max price", "step size")
priors_table %>%
  kable(caption="Price values for Experiment 2.", align="rrr")
```

## Methods

Each participant saw all 5 objects, which were presented in random order. For each object, participants saw the statement:

* [NAME] bought an [OBJECT]. Please rate how likely it is that the cost of the watch is within each of the following ranges.

Sliders corresponding to each price range were arranged in order from lowest price range to highest. Sliders were initialized in white, with a gray handle at the half-way mark. We required participants to drag the handle in order to register their response -- from "Extremely unlikely" (0) to "Extremely Likely" (1). Once participants dragged the slider, the handle and slider changed color to blue. They submitted all of their responses for a given object at once.

## Results

We normalize slider ratings within each participant and object category, since slider ratings likely reflect relative rather than absolute probabilities [@franke_what_2016].

```{r priors figure, fig.width=10, fig.height=2, fig.cap="Experiment 2 results. Error bars are 95% confidence intervals."}
 breaks_fun <- function(x) {
  if (max(x) < 300) {
    c(0, 100, 200)
  } else if (max(x) < 400) {
    c(0, 100, 200, 300)
  } else {
    c(0, 1000, 2000)
  }
}
df12 %>%
  ggplot(aes(x=mid, y=normed_response)) +
  stat_summary(geom="ribbon", fun.data="mean_cl_boot", alpha=0.5, fill="darkgrey") +
  stat_summary(geom="line", fun.y="mean", alpha=1, colour="darkgrey") +
  facet_wrap(~item, scales="free", ncol=5) +
  xlab("Price value") +
  ylab("Normalized rating") +
  # theme(axis.text.x = element_text(angle = -30, hjust = 0))
  scale_x_continuous(breaks = breaks_fun, limits = c(0, NA))
```

Resuts of Experiment 2 are shown in Figure 3. As stated earlier, we chose our price ranges so that we would get very low endorsements for the higher-price bins and relatively smooth curves to the distributions, which we do in fact see in participant responses.

```{r}
get_priors_cors = function(df) {
  df %>%
  select(item, mid, workerid, response) %>%
  spread(workerid, response) %>%
  select(-c(item, mid)) %>%
  cor() %>%
  .[upper.tri(.,diag = F)] %>%
  mean_cl_boot()
}
priors_cors = get_priors_cors(df12)
# df12 %>% group_by(item) %>%
#   do(get_priors_cors(.)) %>%
#   ungroup()
```

Participants tended to be in agreement. The average correlation between participants' responses was `r priors_cors$y` (CI: [`r priors_cors$ymin`, `r priors_cors$ymax`]), and correlations were similarly high within each object category.

```{r load priors fit}
#50K lag 10 worked before...
m12 = read.csv("../models/results/final_expts_results-S1-sigmax10_20000_burn10000_lag10_chain1_inductive_version_s1_inductive_bins_fit_cost_param_listener0_normed_rating_ignore_last_bin.csv",
  col.names = c("result_type", "variable",
                "IGNORE", "object",
                "value", "probability")) %>%
  filter(result_type == "price_prior")
# m12 %>% ggplot(aes(x=value)) +
#   geom_histogram(bins=30) +
#   facet_wrap(variable ~ object, scales="free")
```

```{r reformat priors fit, fig.width=10, fig.height=3}
quantile_errorbars = function(x) {
  return(data.frame(
    y = mean(x),
    ymin = quantile(x, 0.025),
    ymax = quantile(x, 0.975)
  ))
}
m12_params = m12 %>%
  select(-IGNORE) %>%
  filter(result_type == "price_prior") %>%
  group_by(result_type, variable, object) %>%
  mutate(sample = 1:length(value)) %>%
  ungroup() %>%
  filter(result_type=="price_prior") %>%
  spread(variable, value) %>%
  group_by(object) %>%
  mutate(sigma = mean(sigma)) %>%
  group_by(object, sigma) %>%
  do(quantile_errorbars(.$mu)) %>%
  ungroup() %>%
  gather("region", "mu", c(y, ymin, ymax))
dfm12 = reformatted_df12 %>%
  group_by(object, LB, UB) %>%
  do(mean_cl_boot(.$normed_rating)) %>%
  gather("region", "Prior Elicitation", c(y, ymin, ymax)) %>%
  merge(m12_params) %>%
  mutate(Model = mapply(function(ub, lb, m, s) {
    plnorm(ub, meanlog=m, sdlog=s) - plnorm(lb, meanlog=m, sdlog=s)
  }, UB, LB, mu, sigma)) %>%
  select(-c(sigma, mu)) %>%
  gather("src", "value", c(`Prior Elicitation`, Model)) %>%
  spread(region, value) %>%
  mutate(x = (UB + LB)/2)
```

```{r plot priors fit, fig.width=10, fig.height=2, fig.cap="Log-normal fit for Experiment 2. Error bars are 95% confidence intervals."}
# breaks_fun <- function(x) {
#   if (max(x) < 300) {
#     c(0, 100, 200)
#   } else if (max(x) < 400) {
#     c(0, 100, 200, 300)
#   } else {
#     c(0, 1000, 2000)
#   }
# }
# dfm12 %>%
#   group_by(object, src) %>%
#   mutate(ytotal = sum(y),
#          nx = length(x)) %>%
#   ungroup() %>%
#   # group_by(object, src) %>%
#   # summarise(m = mean(nx), mx = max(nx), mn = min(nx))
#   gather("region", "value", c(y, ymin, ymax)) %>%
#   mutate(value = value / ytotal) %>%
#   spread(region, value) %>%
#   mutate(src = factor(src, levels=c("Prior Elicitation", "Model"))) %>%
#   ggplot(aes(x = x, y = y, ymin = ymin, ymax = ymax, colour = src, fill = src,
#                   linetype = src)) +
#   geom_ribbon(alpha=0.5, colour=NA) +
#   geom_line(alpha=1) +
#   facet_wrap(~object, ncol=5, scales="free") +
#   scale_y_continuous(breaks=c()) +
#   scale_x_continuous(breaks=breaks_fun) +
#   ylab("Density") +
#   xlab("Price Value") +
#   scale_fill_manual(values = c("darkgray", "black")) +
#   scale_colour_manual(values = c("darkgray", "black"))
```

```{r cdf cor priors}
sum_lower = function(df) {
  df$value = mapply(function(p, x) {
    sum(df$value[df$x < x]) %>% return()
  }, df$value, df$x)
  return(df)
}
cdf12 = dfm12 %>%
  gather("region", "value", c(y, ymin, ymax)) %>%
  group_by(object, region, src) %>%
  do(sum_lower(.))
priors_fit_cor = cdf12 %>%
  filter(region=="y") %>%
  spread(src, value) %>%
  with(cor(`Model`, `Prior Elicitation`)^2) %>%
  round(3)
```

Normalized slider ratings are very well-fit by log-normal distributions (see Appendix A for inference details). The empirical CDF is highly correlated with its best-fit log-normal CDF ($R^2 = `r priors_fit_cor`$).

```{r cdf plots priors, fig.width=15, fig.height=3}
# cdf12 %>%
#   spread(region, value) %>%
#   ggplot(aes(x=x, y=y, ymin=ymin, ymax=ymax,
#              fill=src)) +
#   geom_line(aes(colour=src)) +
#   geom_ribbon(alpha=1/5) +
#   facet_wrap(~object, ncol = 5, scales="free") +
#   scale_colour_solarized() +
#   scale_fill_solarized() +
#   geom_hline(yintercept = 1, linetype="dashed", colour="black", alpha=1/5) +
#   ylab("Probability") +
#   xlab("Price")
```

# Model

## Background

We base our model of the sorites premise endorsements on @lassiter_adjectival_2015, which uses a scalar adjective model [@lassiter_context_2013] to get a graded endorsement value for sorites. This model of scalar adjectives is a Rational Speech Act model [@frank_predicting_2012;@goodman_knowledge_2013], which makes use of pragmatic principles [@clark_using_1996;@grice_logic_1975;@levinson_interactional_1995] to resolve vague language in context. In this model, the literal semantics of a scalar adjective is relative to some unspecified threshold.

$$[[\mbox{X is expensive}]]_\theta = \mbox{price}(X) > \theta$$

The literal listener jointly infers the threshold $\theta$ and the value along the scalar adjective dimension, in our case, the price $x$ of the "expensive" item $X$.

$$L_0(x, \theta|\mbox{X is expensive}) \propto P(x)P(\theta)\cdot\delta_{x > \theta}$$

Given a particular price value, the speaker chooses whether to endorse (utterance $u$="yes") or deny (utterance $u$="no") the statement "X is expensive" by soft-maximizing their utility of balancing informativity (the literal listener's ability to infer the correct price, marginalized over their interpretation of $\theta$) and the cost of the utterance.

$$U_S(u; x) = \log\left(\int_\theta L_0(x, \theta|u)d\theta\right) - \mbox{cost}(u)$$

The main task in Experiment 1 was to choose how much to endorse a statement. We model this task as a speaker's choice between two alternative utterances: producing the given utterance to a naive listener, or staying silent [@degen_lost_2014;@franke_typical_2014].

## Concrete premise

The concrete premise in our experiment ("An [OBJECT] that costs \$[PRICE VALUE] is expensive") contains a relative clause. However, we model the endorsement task as choosing between uttering the main clause "The [OBJECT] is expensive," or staying silent, given that the content of the relative clause ("The [OBJECT] costs \$[PRICE VALUE]") is true. Hence, the concrete premise is exactly the scalar adjective utterance of @lassiter_context_2013's model.

{{SHOW COR BETWEEN BOTH PHRASINGS?}}

## Inductive premise

The meaning of the inductive premise is less straightforward. When someone states that "A watch that costs \$3 less than an expensive watch is expensive," what information is assumed to be in common ground, and what information is the speaker trying to communicate? Since the price of the "expensive watch" is unknown to participants, how would they know whether or not to say that a less expensive watch is "still expensive"?

Rather than model the sorites inductive premise as a speaker endorsement task, @lassiter_adjectival_2015 look at the _listener_'s joint posterior distribution over the price of the object $x$ and the threshold $\theta$ given the utterance "an expensive [OBJECT]" and directly compute the marginal probability that $x - \varepsilon > \theta$ is true.

We extend @lassiter_adjectival_2015's approach slightly and compute instead a speaker's probability of communicating that a less expensive item is "still expensive" given a listener's joint posterior over $x$ and $\theta$. That is, we assume participants take the following statements as given:

* An [OBJECT] is expensive.
* Another [OBJECT] costs \$[PRICE VALUE] less.

Similar to @lassiter_adjectival_2015, we use a listener model to simultaneously compute the threshold $\theta$ and the price $x_1$ of the "expensive" object (and consequently the price $x_2$ of the "less expensive" object).

$$\begin{aligned}&L_0(x_1,\theta|X_1\mbox{ is expensive}) \\
&x_2 = x_1 - \varepsilon\end{aligned}$$

However, in our model of the inductive premise, the listener then becomes a speaker. The speaker has the goal to communicate the less expensive price $x_2$ to a naive listener (who knows the threshold but not the price), and must choose whether to make the following statement or stay silent:

* [THE LESS EXPENSIVE OBJECT] is expensive.

We therefore model this naive listener's inferred price distribution with vs. without the statement that the less expensive item would be "still expensive":

$$\begin{aligned}L_0(x_2 | X_2 \mbox{ is expensive}, \theta) &=
\frac{Pr(x_2)\delta_{x_2 > \theta}}{\int_\theta Pr(x)\delta_{x > \theta} d\theta} \\ &=
\left\{\begin{array}{ll}\frac{Pr(x_2)}{1 - {CDF}(\theta)} & \mbox{if } \ x_2 > \theta \\ 0 & \mbox{otherwise}\end{array}\right\}\end{aligned}$$

$$L_0(x_2 | \mbox{silent}, \theta) = Pr(x_2)$$

If the cost of the "still expensive" utternace is $-\frac{1}{\lambda}\log(C_X)$ and the cost of the staying silent is $-\frac{1}{\lambda}\log(C_S)$, the speaker endorsements are:

$$\begin{aligned}S_1(u|x_2, \theta) = \frac{\exp(\lambda \left( \ln(L_0(x_2|u, \theta)) - c(u)\right))}{\sum_{u'} \exp(\lambda\left( \ln(L_0(x_2|u', \theta))-c(u') \right))} = \frac{ C_u \left( L_0(x_2|u, \theta)\right)^\lambda}{\sum_{u'} C_{u'} \left(L_0(x_2|u', \theta)\right)^\lambda}\end{aligned}$$

So for the "still expensive" utterance:

$$\begin{aligned}S_1(X_2 \mbox{ is expensive}|x_2, \theta) &=
\frac{C_X \left(L_0(x_2|X_2 \mbox{ is expensive}, \theta)\right)^\lambda}
{C_X \left(L_0(x_2|X_2\mbox{ is expensive}, \theta)\right)^\lambda + C_S \left(L_0(x_2|\mbox{silent}, \theta)\right)^\lambda} \\
&= 
\left\{\begin{array}{ll}
\frac{C_X \left(\frac{Pr(x_2)}{1 - {CDF}(\theta)}\right)^\lambda}
{C_X \left(\frac{Pr(x_2)}{1 - {CDF}(\theta)}\right)^\lambda + C_S \left(Pr(x_2)\right)^\lambda} & \mbox{if } \ x_2  > \theta \\ 0 & \mbox{otherwise}\end{array}\right\} \\
&= 
\left\{\begin{array}{ll}
\frac{C_X}{C_X + C_S\left(1 - {CDF}(\theta)\right)^\lambda} & \mbox{if } \ x_2 > \theta \\ 0 & \mbox{otherwise}
\end{array}\right\}\end{aligned}$$

Finally, we average over participant's initial guess of $x_1$ and $\theta$ to get the expected endorsement.

$$\begin{aligned}S_1(\mbox{still expensive}) = \int_{x,\theta} L_1(x_1,\theta|\mbox{x is expensive})
\left\{\begin{array}{ll}
  \frac{C_X}{C_X + C_S\left(1 - {CDF}(\theta + \varepsilon)\right)^\lambda} & \mbox{if } \ x_1 - \varepsilon > \theta \\
  0  & \text{otherwise.}
\end{array}\right\}
dxd\theta\end{aligned}$$

If the costs of the two utterances are equal, then the $S_1(\mbox{still expensive}|x, \theta)$ value is always between $1/2$ and $1$ whenever $x_1 -\varepsilon > \theta$. In this case, the expected endorsement is very similar to simply oberving the joint distribution $L_0(x_1, \theta | X_1 \mbox{ is expensive})$ and taking the expectation of $x_1-\varepsilon > \theta$. But the smaller $\theta$ is, the less informative the "still expensive" utterance would be, and so the less likely it is that a speaker would actually endorse it over staying silent.
<!-- As the cost of the "still expensive" utterance goes up relative to staying silent, the speaker is even less likely to endorse the utterance for small values of $\theta$. -->

## Analysis

Given the prior distributions over prices for object categories (elicited in Experiment 2), to model responses in Experiment 1, we need only 2 parameters: speaker optimality $\lambda$ and the cost of the utterances. For simplicity, we assume that both utterances have equal cost in this task (similar to giving a simple "yes" or "no" response), leaving the single parameter for speaker optimality. We use Bayesian methods to jointly model Experiments 1 and 2, simultaneously inferring latent parameters of the object price distributions and speaker optimality. Averaging over the posterior distribution of latent parameters, we show how much variance in the endorsement data we can explain by the pragmatic model.

We model the prior distribution over prices as log-normal, with each object category $o$ having its own mean $\mu_o$ and standard deviation $\sigma_o$. We model slider responses for each bin in Experiment 2 as Gaussian, centered at the probability of that bin with standard deviation $\sigma_{\mbox{bin}}$.^[This is a deviation from @franke_what_2016's analysis of binned histogram data which used a logit transform in the linking function from probability to slider ratings.] We put uninformative priors over the price distribution parameters $\mu_o \sim \mbox{Uniform}(0, 10), \sigma_o \sim \mbox{Uniform}(0, 10)$, the response parameter $\sigma_{\mbox{bin}} \sim \mbox{Uniform}(0, 0.5)$, and the speaker model parameter $\lambda \sim \mbox{Uniform}(0, 10)$.

Details of how we carried out this Bayesian inference are described in Appendix A.

## Results

```{r fn to fill in samples}
fill_in_samples = function(df) {
  df %>%
    merge(data.frame(value = rep(df$value[[1]], df$freq[[1]])))
}
```

```{r load full model fit bda}
# mfit = read.csv(
#   "../models/results/final_expts_results-S1-sigmax10_10000_burn5000_lag10_chain11_inductive_version_s1_inductive_concrete_inductive_bins_no_cost_param_listener0_normed_rating_ignore_last_bin.csv",
#   col.names = c("result_type", "variable",
#                 "IGNORE", "object",
#                 "value", "probability"))
# mfit = mfit %>%
#   mutate(freq = probability*1000) %>%
#   rowwise() %>%
#   do(fill_in_samples(.)) %>%
#   ungroup()
# write.csv(mfit, file = "mfit_bda.csv", row.names = F)
mfitbda = read.csv("mfit_bda.csv")
# mfit %>%
#   filter(result_type %in% c("global_param", "price_prior")) %>%
#   ggplot(aes(x=value)) +
#   geom_histogram(bins=30) +
#   facet_wrap(variable ~ object, scales="free")
```

```{r reformat full model fit (priors), fig.width=10, fig.height=3}
mfit_params = mfitbda %>%
  select(-IGNORE) %>%
  filter(result_type == "price_prior") %>%
  group_by(result_type, variable, object) %>%
  mutate(sample = 1:length(value)) %>%
  ungroup() %>%
  filter(result_type=="price_prior") %>%
  spread(variable, value) %>%
  group_by(object) %>%
  mutate(sigma = mean(sigma)) %>%
  group_by(object, sigma) %>%
  do(quantile_errorbars(.$mu)) %>%
  ungroup() %>%
  gather("region", "mu", c(y, ymin, ymax))
dfmfit12 = reformatted_df12 %>%
  group_by(object, LB, UB) %>%
  do(mean_cl_boot(.$normed_rating)) %>%
  gather("region", "Prior Elicitation", c(y, ymin, ymax)) %>%
  merge(mfit_params) %>%
  mutate(Model = mapply(function(ub, lb, m, s) {
    plnorm(ub, meanlog=m, sdlog=s) - plnorm(lb, meanlog=m, sdlog=s)
  }, UB, LB, mu, sigma)) %>%
  select(-c(sigma, mu)) %>%
  gather("src", "value", c(`Prior Elicitation`, Model)) %>%
  spread(region, value) %>%
  mutate(x = (UB + LB)/2)
```

```{r plot full model fit (priors), fig.width=10, fig.height=2, fig.cap="Posterior price distributions fit to Experiments 1 and 2. Error bars are 95% confidence intervals."}
breaks_fun <- function(x) {
  if (max(x) < 300) {
    c(0, 100, 200)
  } else if (max(x) < 400) {
    c(0, 100, 200, 300)
  } else {
    c(0, 1000, 2000)
  }
}
dfmfit12 %>%
  group_by(object, src) %>%
  mutate(ytotal = sum(y),
         nx = length(x)) %>%
  ungroup() %>%
  # group_by(object, src) %>%
  # summarise(m = mean(nx), mx = max(nx), mn = min(nx))
  gather("region", "value", c(y, ymin, ymax)) %>%
  mutate(value = value / ytotal) %>%
  spread(region, value) %>%
  mutate(src = factor(src, levels=c("Prior Elicitation", "Model"))) %>%
  ggplot(aes(x = x, y = y, ymin = ymin, ymax = ymax, colour = src, fill = src,
                  linetype = src)) +
  geom_ribbon(alpha=0.5, colour=NA) +
  geom_line(alpha=1) +
  facet_wrap(~object, ncol=5, scales="free") +
  scale_y_continuous(breaks=c()) +
  scale_x_continuous(breaks=breaks_fun) +
  ylab("") +
  xlab("Price Value") +
  scale_fill_manual(values = c("darkgray", "black")) +
  scale_colour_manual(values = c("darkgray", "black"))
```

```{r cdf cor full model fit}
sum_lower = function(df) {
  df$value = mapply(function(p, x) {
    sum(df$value[df$x < x]) %>% return()
  }, df$value, df$x)
  return(df)
}
cdf12_full = dfmfit12 %>%
  gather("region", "value", c(y, ymin, ymax)) %>%
  group_by(object, region, src) %>%
  do(sum_lower(.))
priors_fit_cor_full_model = cdf12_full %>%
  filter(region=="y") %>%
  spread(src, value) %>%
  with(cor(`Model`, `Prior Elicitation`)^2) %>%
  round(3)
```

```{r priors split half}
# all_workers = df12$workerid %>% unique()
# split_half_cor_priors = function(df) {
#   return(function(x) {
#     half = sample(
#       all_workers,
#       size = length(all_workers)/2,
#       replace = F
#     )
#     df %>% mutate(half = ifelse(workerid %in% half, "A", "B")) %>%
#       mutate(x = lower) %>%
#       group_by(half, x, item) %>%
#       summarise(value = mean(normed_response)) %>%
#       ungroup() %>%
#       group_by(half, item) %>%
#       do(sum_lower(.)) %>%
#       ungroup() %>%
#       spread(half, value) %>%
#       with(., cor(A, B)^2)
#   })
# }
# n_resamples = 1000
# resamples = sapply(1:n_resamples, split_half_cor_priors(df12))
# write.csv(resamples, "resamples.csv", row.names = F)
resamples = read.csv("resamples.csv")$x
split_half_conf_priors = quantile(resamples, probs = c(0.025, 0.975)) %>% round(3) %>% unname()
split_half_correlation_priors = mean(resamples) %>% round(3)
```

The posterior price distributions inferred by fitting to both Experiments 1 and 2 (shown in Figure 4) are very close to the normalized ratings from Experiment 1 (comparing empirical vs. model CDF, $R^2 = `r priors_fit_cor_full_model`$).

```{r reformat full model fit (sorites)}
dfmfit = mfitbda %>% filter(result_type %in% c("S1", "Inductive")) %>%
  group_by(result_type, object, variable) %>%
  do(quantile_errorbars(.$value)) %>%
  ungroup() %>%
  gather("region", "value", c(y, ymin, ymax)) %>%
  mutate(qtype = ifelse(result_type=="Inductive", "inductive", "concrete")) %>%
  rename(dollar_amount = variable) %>%
  mutate(dollar_amount = num(dollar_amount)) %>%
  mutate(src = "Model") %>%
  select(qtype, object, dollar_amount, region, value, src) %>%
  rbind(
    df11 %>% group_by(qtype, object, dollar_amount) %>%
      do(mean_cl_boot(.$response)) %>%
      ungroup() %>%
      mutate(src = "Sorites Experiment") %>%
      gather("region", "value", c(y, ymin, ymax)) %>%
      mutate(dollar_amount = num(dollar_amount)) %>%
      select(qtype, object, dollar_amount, region, value, src)) %>%
  spread(region, value)
```

```{r sorites fit figure, fig.width=10, fig.height=4, fig.cap="Results of Experiment 1. Error bars are 95% confidence intervals."}
 breaks_fun <- function(y) {
  if (max(y) < 2) {
    seq(0, 1, 0.2)
  } else {
    1:9
  }
 }
limits_fun <- function(y) {
  if (max(y) < 2) {
    c(0.9, 9.1)
  } else {
    c(0, 1.05)
  }
}
dfmfit %>%
  # gather("region", "value", c(y, ymin, ymax)) %>%
  # mutate(value = ifelse(src=="Model", num(value)*8+1, num(value))) %>%
  # spread(region, value) %>%
  mutate(src = factor(src, levels=c("Sorites Experiment", "Model"))) %>%
  ggplot(aes(x=dollar_amount, y=y, ymax=ymax, ymin=ymin, colour=qtype, fill=qtype, linetype=qtype, shape=qtype)) +
  geom_errorbar() +
  geom_point() +
  facet_grid(src~object, scales="free") +
  # scale_y_continuous(breaks=1:9, limits=c(0.9, 9.1)) +
  # scale_x_continuous(breaks=breaks_fun) +
  scale_y_continuous(breaks=breaks_fun) +#, limits=limits_fun) +
  xlab("Dollar Amount") +
  ylab("Endorsement Rating") +
  scale_fill_manual(values = c("darkgray", "black")) +
  scale_colour_manual(values = c("darkgray", "black"))
```

```{r sorites fit cors}
concrete_cor = dfmfit %>% select(-c(ymax, ymin)) %>%
  spread(src, y) %>%
  filter(qtype=="concrete") %>%
  with(cor(Model, `Sorites Experiment`)^2 %>% round(3))
inductive_cor = dfmfit %>% select(-c(ymax, ymin)) %>%
  spread(src, y) %>%
  filter(qtype=="inductive") %>%
  with(cor(Model, `Sorites Experiment`)^2 %>% round(3))
```

```{r sorites cors figure, fig.width=8, fig.height=3}
dfmfit %>%
  gather("region", "value", c(y, ymin, ymax)) %>%
  mutate(variable = paste(src, region, sep=".")) %>%
  select(-c(src, region)) %>%
  spread(variable, value) %>%
  ggplot(aes(x=Model.y, xmin=Model.ymin, xmax=Model.ymax,
             y=`Sorites Experiment.y`,
             ymin=`Sorites Experiment.ymin`,
             ymax=`Sorites Experiment.ymax`,
             colour=object, shape=object)) +
  geom_errorbarh() +
  geom_pointrange() +
  scale_colour_grey() +
  facet_wrap(~qtype, scales="free") +
  # scale_y_continuous(breaks=1:9) +
  # xlim(0,1) +
  ylab("Sorites Endorsements") +
  xlab("Model Fit")
```

```{r sorites split half}
# all_workers = df11$workerid %>% unique()
# split_half_cor = function(df) {
#   return(function(x) {
#     half = sample(
#       all_workers,
#       size = length(all_workers)/2,
#       replace = F
#     )
#     df %>% mutate(half = ifelse(workerid %in% half, "A", "B")) %>%
#       group_by(half, qtype, object, dollar_amount) %>%
#       summarise(response = mean(response)) %>%
#       spread(half, response) %>%
#       with(., cor(A, B)) %>%
#       .^2
#   })
# }
# n_resamples = 1000
# resamples_all = sapply(1:n_resamples, split_half_cor(df11))
# write.csv(resamples_all, "resamples_all.csv", row.names = F)
# resamples_concrete = sapply(1:n_resamples, split_half_cor(df11 %>% filter(qtype=="concrete")))
# write.csv(resamples_concrete, "resamples_concrete.csv", row.names = F)
# resamples_inductive = sapply(1:n_resamples, split_half_cor(df11 %>% filter(qtype=="inductive")))
# write.csv(resamples_inductive, "resamples_inductive.csv", row.names = F)
resamples_all = read.csv("resamples_all.csv")$x
resamples_concrete = read.csv("resamples_concrete.csv")$x
resamples_inductive = read.csv("resamples_inductive.csv")$x
split_half_conf = quantile(resamples_all, probs = c(0.025, 0.975)) %>% round(3) %>% unname()
split_half_correlation = mean(resamples_all) %>% round(3)

split_half_conf_concrete = quantile(resamples_concrete, probs = c(0.025, 0.975)) %>% round(3) %>% unname()
split_half_correlation_concrete = mean(resamples_concrete) %>% round(3)

split_half_conf_inductive = quantile(resamples_inductive, probs = c(0.025, 0.975)) %>% round(3) %>% unname()
split_half_correlation_inductive = mean(resamples_inductive) %>% round(3)

# concrete_cor
# split_half_conf_concrete
# inductive_cor
# split_half_conf_inductive
```

The posterior endorsement levels (expected probability of choosing the utterance) for Experiment 2 as well as participants actual responses are shown in Figures 5 and 6. The correlation between participants' mean responses and the model's mean endorsement is very high ($R^2=`r inductive_cor`$ for the inductive premise and $R^2=`r concrete_cor`$ for the concrete premise). These values are close to the split-half correlation (for inductive, mean: `r split_half_correlation_inductive`, CI: [`r split_half_conf_inductive[1]`, `r split_half_conf_inductive[2]`]; for concrete, mean: `r split_half_correlation_concrete`, CI: [`r split_half_conf_concrete[1]`, `r split_half_conf_concrete[2]`]), which represents a ceiling on explainability. For the "watches" object category, we find that participants' endorsements of the concrete premise "A watch that costs \$[PRICE VALUE] is expensive," are noticeably higher than the model's.

```{r calculate map alpha}
# mfit %>%
#   filter(variable == "speakerOptimality") %>%
#   ggplot(aes(x=value)) +
#   geom_histogram()
#   
#   
map_alpha = data.frame(x = seq(0, 1, 0.01)) %>%
  mutate(y = (mfitbda %>%
                filter(variable == "speakerOptimality") %>%
                .$value %>%
                ecdf())(x)) %>%
  filter(abs(y-0.5)<0.01) %>%
  .$x[[1]] %>% round(3)
mean_alpha = mfitbda %>%
  filter(variable == "speakerOptimality") %>%
  .$value %>% mean() %>% round(3)
```

```{r load full model fit map alpha}
# mfit = read.csv(
#   "../models/results/final_expts_results-S1-sigmax10_10000_burn5000_lag10_chain1_inductive_version_s1_inductive_bins_no_cost_param_map_alpha_listener0_normed_rating_ignore_last_bin.csv",
#   col.names = c("result_type", "variable",
#                 "IGNORE", "object",
#                 "value", "probability"))
# mfit = mfit %>%
#   mutate(freq = probability*1000) %>%
#   rowwise() %>%
#   do(fill_in_samples(.)) %>%
#   ungroup()
# write.csv(mfit, file = "mfit_alpha.csv", row.names = F)
mfit_alpha = read.csv("mfit_alpha.csv")
# mfit %>%
#   filter(result_type %in% c("global_param", "price_prior")) %>%
#   ggplot(aes(x=value)) +
#   geom_histogram(bins=30) +
#   facet_wrap(variable ~ object, scales="free")
```

```{r reformat full model fit alpha (sorites)}
dfmfit_alpha = mfit_alpha %>% filter(result_type %in% c("S1", "Inductive")) %>%
  group_by(result_type, object, variable) %>%
  do(quantile_errorbars(.$value)) %>%
  ungroup() %>%
  gather("region", "value", c(y, ymin, ymax)) %>%
  mutate(qtype = ifelse(result_type=="Inductive", "inductive", "concrete")) %>%
  rename(dollar_amount = variable) %>%
  mutate(dollar_amount = num(dollar_amount)) %>%
  mutate(src = "Model") %>%
  select(qtype, object, dollar_amount, region, value, src) %>%
  rbind(
    df11 %>% group_by(qtype, object, dollar_amount) %>%
      do(mean_cl_boot(.$response)) %>%
      ungroup() %>%
      mutate(src = "Sorites Experiment") %>%
      gather("region", "value", c(y, ymin, ymax)) %>%
      mutate(dollar_amount = num(dollar_amount)) %>%
      select(qtype, object, dollar_amount, region, value, src)) %>%
  spread(region, value)
```

```{r sorites fit cors alpha}
concrete_cor_alpha = dfmfit_alpha %>% select(-c(ymax, ymin)) %>%
  spread(src, y) %>%
  filter(qtype=="concrete") %>%
  with(cor(Model, `Sorites Experiment`)^2 %>% round(3))
inductive_cor_alpha = dfmfit_alpha %>% select(-c(ymax, ymin)) %>%
  spread(src, y) %>%
  filter(qtype=="inductive") %>%
  with(cor(Model, `Sorites Experiment`)^2 %>% round(3))
```

There are 11 parameters necessary for modeling Experiment 2 (5 mean and 5 variance parameters for the object categories and one variance parameter for the slider responses) and only one parameter necessary for modeling Experiment 1. Since the fit for Experiment 1 is our primary interest, we want to check how much the Experiment 2 parameters are being used to fit Experiment 1's data. We therefore set the optimality parameter to the the maximum a posteriori estimate under the full Bayesian data analysis ($\lambda = `r map_alpha`$), and directly compute responses to Experiment 1, fitting the object distribution parameters to Experiment 2's data only. This gives similar correlations with the empirical responses in Experiment 1 ($R^2=`r inductive_cor_alpha`$ for inductive and $R^2 = `r concrete_cor_alpha`$ for concrete).

## Discussion

{{WHY ARE WATCHES WEIRD?}}

{{DISCUSS FREE PARAMS}}

{{SUMMARIZE TAKE-AWAYS}}

# General Discussion

* point
    * point
    * point

* point
    * point
    * point

* point
    * point
    * point

* point
    * point
    * point

* point
    * point
    * point

# References

\appendix

# Inference details

The code for our model and procedures and data from our experiments can be found at {{URL}}. We implemented our model in the probabilistic programming language WebPPL [@goodman_design_2014]. We used an incrementalized version of the Metropolis- Hastings algorithm [@ritchie_c3:_2016]. We discarded the first 5000 samples for burn in. After burn in, we kept every 10th sample. In total, we kept 10000 samples to represent the posterior distribution.

We set uninformative priors over the hyperparameters for the price distribution parameters $\mu_o \sim \mbox{Uniform}(0, 10), \sigma_o \sim \mbox{Uniform}(0, 10)$, the response parameter $\sigma_{\mbox{bin}} \sim \mbox{Uniform}(0, 0.5)$, and the speaker model parameter $\lambda \sim \mbox{Uniform}(0, 10)$.

### Discretization

Since inference over the speaker model has a nested inference of the literal listener distribution, we used a simplification. For each continuous sample of the set of prior parameters $\mu_o$ and $\sigma_o$, we discretized the domain of these priors and computed exact probability distributions for the prior on prices.

We chose a discretization that would have few bins, but would also closely resemble the behavior of the true distributions. Since the range of prices is so wide, and since some prices are very close together, it would be difficult to create a discretization that works for every dollar amount used across all experiments. Because of this, we implemented a separate discretization for each experiment.

For discretizing the prior, we created a set of bins (not necessarily of equal width) such that each price in the experiment falls into a unique bin. We also want there to be bins below the lowest price’s bin and above the highest price’s bin. The probability of a price being sampled from each bin is computed from the CDF: `p_bin = pnorm(upper_boundary) - pnorm(lower_boundary)`.

The distribution of the threshold $\theta$ was also discretized and represented relative to the prior bins. We assumed the true distribution of $\theta$ to be independent of $x$ and uniform, with maximum value at 10% higher than the largest dollar amount used for that item in the experiment. Since the bins of of different widths, the probability of $\theta$ falling between prices from adjacent bins varied, depending on the bins. We used a simple scaling based on the width of the bins. This is an approximation to the true joint distribution over prices $x$ and thresholds $\theta$.

The discretization used for the full set of sorites experiments is shown in Figure 7. The height of the bars represent the probability that theta will fall between each pair of midpoints. The midpoints of the bins appear in black. Bins are shown divided by vertical lines. The representative theta values, which appear at the boundaries of the bins, are shown in grey.

```{r discretization figure, fig.width=10, fig.height=2, fig.cap="Discretization and marginal distribution over the threshold variable $\theta$ for each object category."}

expt_label="11"

rename_list = function(lst, name, new_name) {
  lst[[new_name]] = lst[[name]]
  lst[[name]] = NULL
  return(lst)
}

select_expt = function(lst, expt_label) {
  # for each object
  for (name in names(lst)) {
    lst[[name]] = lst[[name]][[expt_label]]
    lst[[name]][["dollar_amount_lookups"]] = NULL
    lst[[name]][["theta_lookups"]] = NULL
    lst[[name]][["width_lookups"]] = NULL
    lst[[name]][["max_theta"]] = NULL
    lst[[name]]$upper[[length(lst[[name]]$upper)]] = NA
    lst[[name]]$upper = unlist(lst[[name]]$upper)
  }
  do.call(rbind, lapply(names(lst), function(name) {
    lst[[name]] %>% as.data.frame() %>%
      mutate(object = name) %>% return()
  })) %>% return()
}
bins = RJSONIO::fromJSON("../models/results/bins.json") %>%
  rename_list("coffee maker", "coffee_maker") %>%
  select_expt(expt_label) %>%
  gather("variable", "value", -object) %>%
  group_by(object, variable) %>%
  mutate(bin_number = 1:length(value)) %>%
  spread(variable, value)
theta_bins = bins %>%
  select(object, mid, theta_prob) %>%
  rename(upper_theta = mid) %>%
  mutate(lower_theta = c(0, upper_theta[1:(length(upper_theta)-1)])) %>%
  gather("var", "theta", c(upper_theta, lower_theta)) %>%
  select(-var)
p = bins %>%
  ggplot() +
  geom_point(aes(x=mid, y=0)) +
  geom_point(aes(x=theta, y=theta_prob), alpha=0.2) +
  geom_vline(aes(xintercept=lower), alpha=0.2) +
  geom_ribbon(data=theta_bins, aes(x=theta, ymin=0, ymax=theta_prob), alpha=0.2) +
  facet_wrap(~object, ncol = 5, scales="free")
bins %>%
  # filter(object == "laptop") %>%
  ggplot() +
  facet_wrap(~object, ncol = 5, scales="free") +
  geom_point(aes(x=mid, y=0)) +
  geom_point(aes(x=theta, y=theta_prob), alpha=0.2) +
  geom_vline(aes(xintercept=lower), alpha=0.2) +
  geom_ribbon(data=theta_bins, aes(x=theta, ymin=0, ymax=theta_prob), alpha=0.2)
```
