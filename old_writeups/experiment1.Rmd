---
title: "Sorites Experiment 1"
output:
  pdf_document:
    toc: true
    highlight: zenburn
    toc_depth: 3
---

```{r load libraries, echo=F, message=F, warning=F}
library(rjson)
library(fitdistrplus)
library(ggplot2)
library(dplyr)
library(tidyr)
library(grid)
library(ggthemes)
library(languageR)
library(psych)
library(lme4)
library(lmerTest)
library(diagram)
library(boot)
char = as.character
num = function(x) {return(as.numeric(char(x)))}
grab = function(colname) { return(function(lst) { return(unlist(lst[[colname]])) }) }
options(digits=3)
mean.sd = funs(mean, sd)
```

## Description of experiment

A copy of the experiment can be found at: [`experiments/experiment1/prices_8-27.html`](../experiments/experiment1/prices_8-27.html).

This was a second stage of the sorites experiment, which we ran in August of 2013. The design was identical to [Experiment 0](experiment0.pdf), except that we added more extreme values.

**Values V** We chose the dollar amount values of the items to be 0, 1, 2, 3, and 4 standard deviations above the mean prices of the given item category.

**Epsilons E** We chose the dollar amount for the "difference" E in the inductive premise to be 0.01, 0.1, 0.5, 1, 2, and 3 multiples of a standard deviation for the price of the given item category.

For each of the 5 items, there were 5 possible concrete premise sentences and 6 possible inductive premise sentences. In order to keep the number of questions roughly the same as in Experiment 0,  of these 55 possible questions, each participant saw some random subset of 44 of those questions.

## Results

There were 50 participants.

```{r, echo=F, message=F, warning=F, fig.width=8, fig.height=3}
d0 = read.csv('../data/experiment0-and-experiment1.csv') %>%
  filter(num(workerid) >= 30) %>%
  rename(questions=Answer.questions,
         language=Answer.language,
         age=Answer.age,
         comments=Answer.comments) %>%
  mutate(language = factor(sub('"', '', sub('"', '', char(language))))) %>%
  select(workerid, questions, language, age, comments)

# ### at this point, could exclude particpants whose native language was not english:
# d0 = d0 %>% filter(language %in% c('english', 'English', 'ENGLISH'))
# print(unique(d0$comments))
d0 = select(d0, workerid, questions)

questions = sapply(char(d0$questions), fromJSON)
n0 = length(questions)
graball = function(colname) {
  return(unlist(sapply(1:n0, function(i) {
    return((grab(colname))(questions[[i]])) }))) }
d0 = data.frame(qNumber = graball('qNumber'),
                qType = graball('qType'),
                dollarAmt = graball('dollarAmt'),
                sigs = graball('sigs'),
                item = graball('item'),
                response = graball('response'),
                workerid = rep(d0$workerid, rep(44, length(d0$workerid)))) %>%
  mutate(dollarAmt = as.numeric(sub('\\$','',as.character(dollarAmt))),
         response = as.integer(response),
         workerid = as.factor(workerid))

samplemean <- function(x, d) {
  return(mean(x[d]))
}
ci = function(vals) {
  cis = boot.ci(boot(vals, samplemean, 1000), type="bca")
  low = cis$bca[1,4]
  high = cis$bca[1,5]
  return(data.frame(mean=mean(vals),
                    ci.low=low,
                    ci.high=high,
                    N=length(vals)))
}

aggr = d0 %>% group_by(item, dollarAmt, qType) %>%
#   summarise(response = mean(response)) %>%
  do(ci(.$response)) %>%
  rename(response=mean) %>% as.data.frame
ggplot(aggr, aes(x=dollarAmt, y=response, colour=item, linetype=qType)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(x=dollarAmt, ymin=ci.low, ymax=ci.high), width=0) +
  facet_grid(~ item, scale='free') +
  scale_y_continuous(breaks = 1:9, limits=c(1, 9)) +
  theme_few() + scale_colour_few()
```

## Model fit

I ran the [adjectives model](../model/adjectives-lognormal-prior.wppl) on the [prior data Justine collected ages ago](../model/justine-priors-lognormal-fit/prior-params.csv).

I did a grid search for best parameter fit, but there doesn't seem to be a clear pattern in how different cost and rationality parameters affect the model fit...

```{r echo=F, message=F, warning=F, fig.width=5, fig.height=3}
## load model data from webppl output
m0 = do.call(rbind,
             lapply(
               list.files('../model/justine-priors-lognormal-fit/simulation-results/commit-7d785bf97f7d54e55d28dc7a267ec68c91277f2c-run0/',
                          pattern='.*csv$'),
               function(filename) {
                 return(read.csv(
                   paste('../model/justine-priors-lognormal-fit/simulation-results/commit-7d785bf97f7d54e55d28dc7a267ec68c91277f2c-run0/',
                         filename, sep=''),
                   header=F,
                   col.names=c('item', 'cost', 'price', 'theta', 'score', 'probability', 'alpha')))
                 })) %>% filter(item != 'electric.kettle') %>%
  mutate(alpha = num(alpha), cost = num(cost))

## get model ratings for sorites premises

# fn for getting model rating for particular question and parameter settings
get.model.rating = function(relevant.item, relevant.dollarAmt,
                            relevant.alpha, relevant.cost,
                            relevant.qtype) {
  if (relevant.item == 'coffee maker') {relevant.item = 'coffee.maker'}
  relevant.model.data = subset(m0,
                               item == relevant.item &
                                 alpha == relevant.alpha &
                                 cost == relevant.cost)
  stopifnot(sum(relevant.model.data$probability) - 1 < 0.00001)
  if (relevant.qtype == 'eps') {
    conisistent.with.premise = subset(relevant.model.data, (price - relevant.dollarAmt) > theta)
  } else if (relevant.qtype == 'val') {
    conisistent.with.premise = subset(relevant.model.data, relevant.dollarAmt > theta)
  } else {
    print(paste('error 234: relevant.qtype should be either eps or val, but it is:',
                relevant.qtype))
  }
  return(sum(conisistent.with.premise$probability))
}
# enumerate questions and parameter settings and collect model data
alphas = 1:10
costs = 1:10
params = expand.grid(alpha=alphas, cost=costs)
all.qns = select(aggr, item, dollarAmt, qType) %>% mutate(qType = char(qType))
model.ratings = expand.grid(param.set=1:nrow(params), qn.set=1:nrow(all.qns)) %>%
  mutate(alpha = sapply(param.set, function(p.s) {params[p.s,'alpha']}),
         cost = sapply(param.set, function(p.s) {params[p.s,'cost']}),
         item = sapply(qn.set, function(q.s) {all.qns[q.s,'item']}),
         dollarAmt = sapply(qn.set, function(q.s) {all.qns[q.s,'dollarAmt']}),
         qType = sapply(qn.set, function(q.s) {all.qns[q.s,'qType']})) %>%
  select(-param.set, -qn.set) %>%
  mutate(model.rating = mapply(get.model.rating, char(.$item), dollarAmt, alpha, cost, qType))

# get best fit parameters for each question
get.sse.to.data = function(relevant.alpha, relevant.cost) {
  relevant.model.ratings = subset(model.ratings, alpha==relevant.alpha & cost==relevant.cost)
  combo.data = data.frame(cbind(relevant.model.ratings, aggr)) %>%
#   # check data cbind worked
#   stopifnot(mean(c(with(combo.data, mean(qType==qType.1)),
#                   with(combo.data, mean(item==item.1)),
#                   with(combo.data, mean(dollarAmt==dollarAmt.1)))) - 1 < 0.00001)
  select(response, model.rating)
  sse = function(a, b) {return(sum((a-b)^2))}
  return(with(combo.data, sse(response, model.rating)))
}
params = params %>% mutate(sse = mapply(get.sse.to.data, alpha, cost))
ggplot(params, aes(x=alpha, y=cost, fill=sse, colour=sse)) +
  geom_tile() +
  theme_few() + scale_fill_gradient(low='black', high='white') +
  scale_colour_gradient(low='black', high='white') +
  ggtitle('fit to data for different parameters')
best.alpha = params$alpha[params$sse == min(params$sse)][[1]]
best.cost = params$cost[params$sse == min(params$sse)][[1]]
```

The best fit parameters were alpha=`r best.alpha` and cost=`r best.cost`.

Just like in experiment 0, this is a pretty awful fit, since the sorites premises are almost always definitely true according to the model for the dollar amounts we asked about.

This is probably because the prior distributions from the original prior elicitation experiment aren't quite right for sorites (since we care about the tails of the distributions).

This is what we originally thought, which is why we did more prior elicitation experiments.

```{r echo=F, message=F, warning=F, fig.width=8, fig.height=4}
best.fit.model.ratings = subset(model.ratings, alpha==best.alpha & cost == best.cost) %>%
  mutate(src = 'model',
         ci.low=model.rating,
         ci.high=model.rating) %>% rename(response = model.rating) %>%
  select(-alpha, -cost) %>%
  rbind(., (aggr %>% mutate(src='experiment') %>% select(-N)))
ggplot(best.fit.model.ratings, aes(x=dollarAmt, y=response, colour=item, linetype=qType)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(x=dollarAmt, ymin=ci.low, ymax=ci.high), width=0) +
  facet_grid(src ~ item, scale='free') +
  theme_few() + scale_colour_few()
```

Using the prior data from ... we find that ...

```{r echo=F, message=F, warning=F, fig.width=8, fig.height=4}
```