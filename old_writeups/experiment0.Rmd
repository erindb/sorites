---
title: "Sorites Experiment 0"
output:
  pdf_document:
    toc: true
    highlight: zenburn
    toc_depth: 3
---

```{r load libraries, echo=F, message=F, warning=F}
library(rjson)
library(fitdistrplus)
library(ggplot2)
library(dplyr)
library(tidyr)
library(grid)
library(ggthemes)
library(languageR)
library(psych)
library(lme4)
library(lmerTest)
library(diagram)
library(boot)
char = as.character
num = function(x) {return(as.numeric(char(x)))}
grab = function(colname) { return(function(lst) { return(unlist(lst[[colname]])) }) }
options(digits=3)
mean.sd = funs(mean, sd)
```

## Description of experiment

A copy of the experiment can be found at: [`experiments/experiment0/prices_8-27.html`](../experiments/experiment0/prices_8-27.html).

This was the first sorites experiment, in August of 2013. We asked participants to rate the concrete ("An X that costs $V is expensive") and inductive ("An X that costs $E less than an expensive X is also expensive") premises of the sorites paradox.

**Items X** We chose items X to be one of these 5 items: headphones, sweater, laptop, coffee maker, watch.

**Values V** We chose the dollar amount values of the items to be 0, 1, 2, and 3 standard deviations above the mean prices of the given item category.

**Epsilons E** We chose the dollar amount for the "difference" E in the inductive premise to be 0.01, 0.1, 0.5, and 1 multiples of a standard deviation for the price of the given item category.

So for each of the 5 items, each participant saw 4 possible concrete premise sentences and 4 possible inductive premise sentences.

We rounded the prices to the nearest multiple of the nearest round number according to my intuition about a reasonable halo effect (nearest cent for prices less than $1, nearest dollar for prices less than $20, nearest $10 for prices less than $100, and nearest $100 otherwise).

### Estimates of means and standard deviations of prices per item category

The means and standard deviations of the item's prices were estimated based on Justine's initial prior elicitation experiment where participants gave free response estimates of prices (I think that was the design).

These are the values for each of the 5 values in this experiment and for the one other item category that we didn't include (electric kettle).

```{r echo=F, message=F, warning=F}
justine.priors.text = readLines("../experiments/experiment0/prices_8-27_files/humanPriors.js")[[1]]
justine.priors = fromJSON(substr(justine.priors.text, 19, nchar(justine.priors.text)-1)) %>%
  as.data.frame %>%
  gather("item", "price", 1:6)
aggr.priors = justine.priors %>%
  group_by(item) %>%
  summarise_each(mean.sd) %>%
  as.data.frame %>%
  mutate(rounded.sd = c(20, NA, 30, 300, 15, 15),
         rounded.mean = c(40, NA, 30, 600, 30, 30))
print(aggr.priors)
```

For some reason I didn't do this at the very beginning, but I later fit these data to a lognormal distribution and got the following fits:

```{r echo=F, message=F, warning=F, fig.width=8, fig.height=4}
fit.lognorm = function(vals) {
  fit = fitdist(vals, "lnorm")
  fit.estimate.data = as.data.frame(t(fit$estimate))
  return(fit.estimate.data)
}
justine.prior.fits = justine.priors %>% group_by(item) %>%
  do(fit.lognorm(.$price)) %>%
  as.data.frame
print(justine.prior.fits)
  
get.fit.lines = function(item.category) {
  vals = justine.priors$price[justine.priors$item == item.category]
  params = subset(justine.prior.fits, item == item.category)
  x = seq(0, max(vals), length.out=100)
  y = dlnorm(x, params$meanlog[[1]], params$sdlog[[1]])
  return(data.frame(item=item.category, x=x, y=y))
}
justine.prior.fit.lines = do.call(rbind, lapply(as.list(justine.prior.fits$item), get.fit.lines))

ggplot(data=justine.priors, aes(x=price, colour=item, fill=item)) +
  geom_density() +
  facet_wrap(~item, scale='free') +
  geom_line(data=justine.prior.fit.lines, aes(x=x, y=y)) +
  theme_few() + scale_colour_few() + scale_fill_few()
```

## Results

Participants thought most of the prices were expensive, so we decided to use more epsilon values in [the next experiment](experiment1.pdf).

There were 30 participants.

```{r, echo=F, message=F, warning=F, fig.width=8, fig.height=3}
d0 = read.csv('../data/experiment0-and-experiment1.csv') %>%
  filter(num(workerid) < 30) %>%
  rename(questions=Answer.questions,
         language=Answer.language,
         age=Answer.age,
         comments=Answer.comments) %>%
  mutate(language = factor(sub('"', '', sub('"', '', char(language))))) %>%
  select(workerid, questions, language, age, comments)

# ### at this point, could exclude particpants whose native language was not english:
# d0 = d0 %>% filter(language %in% c('english', 'English', 'ENGLISH'))
# print(unique(d0$comments))
d0 = select(d0, workerid, questions)

questions = sapply(char(d0$questions), fromJSON)
n0 = length(questions)
graball = function(colname) {
  return(unlist(sapply(1:n0, function(i) {
    return((grab(colname))(questions[[i]])) }))) }
d0 = data.frame(qNumber = graball('qNumber'),
                qType = graball('qType'),
                dollarAmt = graball('dollarAmt'),
                sigs = graball('sigs'),
                item = graball('item'),
                response = graball('response'),
                workerid = rep(d0$workerid, rep(40, length(d0$workerid)))) %>%
  mutate(dollarAmt = as.numeric(sub('\\$','',as.character(dollarAmt))),
         response = as.integer(response),
         workerid = as.factor(workerid))

samplemean <- function(x, d) {
  return(mean(x[d]))
}
ci = function(vals) {
  cis = boot.ci(boot(vals, samplemean, 1000), type="bca")
  low = cis$bca[1,4]
  high = cis$bca[1,5]
  return(data.frame(mean=mean(vals),
                    ci.low=low,
                    ci.high=high,
                    N=length(vals)))
}

aggr = d0 %>% group_by(item, dollarAmt, qType) %>%
#   summarise(response = mean(response)) %>%
  do(ci(.$response)) %>%
  rename(response=mean) %>% as.data.frame
ggplot(aggr, aes(x=dollarAmt, y=response, colour=item, linetype=qType)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(x=dollarAmt, ymin=ci.low, ymax=ci.high), width=0) +
  facet_grid(~ item, scale='free') +
  scale_y_continuous(breaks = 1:9, limits=c(1, 9)) +
  theme_few() + scale_colour_few()
```

## Model fit

I ran the [adjectives model](../model/adjectives-lognormal-prior.wppl) on the [prior data Justine collected ages ago](../model/justine-priors-lognormal-fit/prior-params.csv).

I did a grid search for best parameter fit, but there doesn't seem to be a clear pattern in how different cost and rationality parameters affect the model fit...

```{r echo=F, message=F, warning=F, fig.width=5, fig.height=3}
## load model data from webppl output
m0 = do.call(rbind,
             lapply(
               list.files('../model/justine-priors-lognormal-fit/simulation-results/commit-7d785bf97f7d54e55d28dc7a267ec68c91277f2c-run0/',
                          pattern='.*csv$'),
               function(filename) {
                 return(read.csv(
                   paste('../model/justine-priors-lognormal-fit/simulation-results/commit-7d785bf97f7d54e55d28dc7a267ec68c91277f2c-run0/',
                         filename, sep=''),
                   header=F,
                   col.names=c('item', 'cost', 'price', 'theta', 'score', 'probability', 'alpha')))
                 })) %>% filter(item != 'electric.kettle') %>%
  mutate(alpha = num(alpha), cost = num(cost))

## get model ratings for sorites premises

# fn for getting model rating for particular question and parameter settings
get.model.rating = function(relevant.item, relevant.dollarAmt,
                            relevant.alpha, relevant.cost,
                            relevant.qtype) {
  if (relevant.item == 'coffee maker') {relevant.item = 'coffee.maker'}
  relevant.model.data = subset(m0,
                               item == relevant.item &
                                 alpha == relevant.alpha &
                                 cost == relevant.cost)
  stopifnot(sum(relevant.model.data$probability) - 1 < 0.00001)
  if (relevant.qtype == 'eps') {
    conisistent.with.premise = subset(relevant.model.data, (price - relevant.dollarAmt) > theta)
  } else if (relevant.qtype == 'val') {
    conisistent.with.premise = subset(relevant.model.data, relevant.dollarAmt > theta)
  } else {
    print(paste('error 234: relevant.qtype should be either eps or val, but it is:',
                relevant.qtype))
  }
  return(sum(conisistent.with.premise$probability))
}
# enumerate questions and parameter settings and collect model data
alphas = 1:10
costs = 1:10
params = expand.grid(alpha=alphas, cost=costs)
all.qns = select(aggr, item, dollarAmt, qType)
model.ratings = expand.grid(param.set=1:nrow(params), qn.set=1:nrow(all.qns)) %>%
  mutate(alpha = sapply(param.set, function(p.s) {params[p.s,'alpha']}),
         cost = sapply(param.set, function(p.s) {params[p.s,'cost']}),
         item = sapply(qn.set, function(q.s) {all.qns[q.s,'item']}),
         dollarAmt = sapply(qn.set, function(q.s) {all.qns[q.s,'dollarAmt']}),
         qType = sapply(qn.set, function(q.s) {all.qns[q.s,'qType']})) %>%
  select(-param.set, -qn.set) %>%
  mutate(model.rating = mapply(get.model.rating, char(.$item), dollarAmt, alpha, cost, qType))

# get best fit parameters for each question
get.sse.to.data = function(relevant.alpha, relevant.cost) {
  relevant.model.ratings = subset(model.ratings, alpha==relevant.alpha & cost==relevant.cost)
  combo.data = data.frame(cbind(relevant.model.ratings, aggr)) %>%
#   # check data cbind worked
#   stopifnot(mean(c(with(combo.data, mean(qType==qType.1)),
#                   with(combo.data, mean(item==item.1)),
#                   with(combo.data, mean(dollarAmt==dollarAmt.1)))) - 1 < 0.00001)
  select(response, model.rating)
  sse = function(a, b) {return(sum((a-b)^2))}
  return(with(combo.data, sse(response, model.rating)))
}
params = params %>% mutate(sse = mapply(get.sse.to.data, alpha, cost))
ggplot(params, aes(x=alpha, y=cost, fill=sse, colour=sse)) +
  geom_tile() +
  theme_few() + scale_fill_gradient(low='black', high='white') +
  scale_colour_gradient(low='black', high='white') +
  ggtitle('fit to data for different parameters')
best.alpha = params$alpha[params$sse == min(params$sse)][[1]]
best.cost = params$cost[params$sse == min(params$sse)][[1]]
```

The best fit parameters were alpha=`r best.alpha` and cost=`r best.cost`.

This is a pretty awful fit, since the sorites premises are almost always definitely true according to the model for the dollar amounts we asked about.

This is probably because the prior distributions from the original prior elicitation experiment aren't quite right for sorites (since we care about the tails of the distributions).

This is what we originally thought, which is why we did more prior elicitation experiments.

```{r echo=F, message=F, warning=F, fig.width=8, fig.height=4}
best.fit.model.ratings = subset(model.ratings, alpha==best.alpha & cost == best.cost) %>%
  mutate(src = 'model',
         ci.low=model.rating,
         ci.high=model.rating) %>% rename(response = model.rating) %>%
  select(-alpha, -cost) %>%
  rbind(., (aggr %>% mutate(src='experiment') %>% select(-N)))
ggplot(best.fit.model.ratings, aes(x=dollarAmt, y=response, colour=item, linetype=qType)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(x=dollarAmt, ymin=ci.low, ymax=ci.high), width=0) +
  facet_grid(src ~ item, scale='free') +
  theme_few() + scale_colour_few()
```

Using the prior data from ... we find that ...

```{r echo=F, message=F, warning=F, fig.width=8, fig.height=4}
```