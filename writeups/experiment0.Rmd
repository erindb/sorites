---
title: "Sorites Experiment 0"
output:
  pdf_document:
    toc: true
    highlight: zenburn
    toc_depth: 3
---

```{r load libraries, echo=F, message=F, warning=F}
library(rjson)
library(fitdistrplus)
library(ggplot2)
library(dplyr)
library(tidyr)
library(grid)
library(ggthemes)
library(languageR)
library(psych)
library(lme4)
library(lmerTest)
library(diagram)
library(boot)
char = as.character
num = function(x) {return(as.numeric(char(x)))}
grab = function(colname) { return(function(lst) { return(unlist(lst[[colname]])) }) }
options(digits=3)
mean.sd = funs(mean, sd)
```

## Description of experiment

A copy of the experiment can be found at: [`experiments/experiment0/prices_8-27.html`](../experiments/experiment0/prices_8-27.html).

This was the first sorites experiment, in August of 2013. We asked participants to rate the concrete ("An X that costs $V is expensive") and inductive ("An X that costs $E less than an expensive X is also expensive") premises of the sorites paradox.

**Items X** We chose items X to be one of these 5 items: headphones, sweater, laptop, coffee maker, watch.

**Values V** We chose the dollar amount values of the items to be 0, 1, 2, and 3 standard deviations above the mean prices of the given item category.

**Epsilons E** We chose the dollar amount for the "difference" E in the inductive premise to be 0.01, 0.1, 0.5, and 1 multiples of a standard deviation for the price of the given item category.

So for each of the 5 items, each participant saw 4 possible concrete premise sentences and 4 possible inductive premise sentences.

We rounded the prices to the nearest multiple of the nearest round number according to my intuition about a reasonable halo effect (nearest cent for prices less than $1, nearest dollar for prices less than $20, nearest $10 for prices less than $100, and nearest $100 otherwise).

### Estimates of means and standard deviations of prices per item category

The means and standard deviations of the item's prices were estimated based on Justine's initial prior elicitation experiment where participants gave free response estimates of prices (I think that was the design).

These are the values for each of the 5 values in this experiment and for the one other item category that we didn't include (electric kettle).

```{r echo=F, message=F, warning=F}
justine.priors.text = readLines("../experiments/experiment0/prices_8-27_files/humanPriors.js")[[1]]
justine.priors = fromJSON(substr(justine.priors.text, 19, nchar(justine.priors.text)-1)) %>%
  as.data.frame %>%
  gather("item", "price", 1:6)
aggr.priors = justine.priors %>%
  group_by(item) %>%
  summarise_each(mean.sd) %>%
  as.data.frame %>%
  mutate(rounded.sd = c(20, NA, 30, 300, 15, 15),
         rounded.mean = c(40, NA, 30, 600, 30, 30))
print(aggr.priors)
```

For some reason I didn't do this at the very beginning, but I later fit these data to a lognormal distribution and got the following fits:

```{r echo=F, message=F, warning=F, fig.width=8, fig.height=4}
fit.lognorm = function(vals) {
  fit = fitdist(vals, "lnorm")
  fit.estimate.data = as.data.frame(t(fit$estimate))
  return(fit.estimate.data)
}
justine.prior.fits = justine.priors %>% group_by(item) %>%
  do(fit.lognorm(.$price)) %>%
  as.data.frame
print(justine.prior.fits)
  
get.fit.lines = function(item.category) {
  vals = justine.priors$price[justine.priors$item == item.category]
  params = subset(justine.prior.fits, item == item.category)
  x = seq(0, max(vals), length.out=100)
  y = dlnorm(x, params$meanlog[[1]], params$sdlog[[1]])
  return(data.frame(item=item.category, x=x, y=y))
}
justine.prior.fit.lines = do.call(rbind, lapply(as.list(justine.prior.fits$item), get.fit.lines))

ggplot(data=justine.priors, aes(x=price, colour=item, fill=item)) +
  geom_density() +
  facet_wrap(~item, scale='free') +
  geom_line(data=justine.prior.fit.lines, aes(x=x, y=y)) +
  theme_few() + scale_colour_few() + scale_fill_few()
```

## Results

Participants thought most of the prices were expensive, so we decided to use more epsilon values in the next experiment.

There were 30 participants.

```{r, echo=F, message=F, warning=F, fig.width=8, fig.height=3}
d0 = read.csv('../data/experiment0-and-experiment1.csv') %>%
  filter(num(workerid) < 30) %>%
  rename(questions=Answer.questions,
         language=Answer.language,
         age=Answer.age,
         comments=Answer.comments) %>%
  mutate(language = factor(sub('"', '', sub('"', '', char(language))))) %>%
  select(workerid, questions, language, age, comments)

# ### at this point, could exclude particpants whose native language was not english:
# d0 = d0 %>% filter(language %in% c('english', 'English', 'ENGLISH'))
# print(unique(d0$comments))
d0 = select(d0, workerid, questions)

questions = sapply(char(d0$questions), fromJSON)
n0 = length(questions)
graball = function(colname) {
  return(unlist(sapply(1:n0, function(i) {
    return((grab(colname))(questions[[i]])) }))) }
d0 = data.frame(qNumber = graball('qNumber'),
                qType = graball('qType'),
                dollarAmt = graball('dollarAmt'),
                sigs = graball('sigs'),
                item = graball('item'),
                response = graball('response'),
                workerid = rep(d0$workerid, rep(40, length(d0$workerid)))) %>%
  mutate(dollarAmt = as.numeric(sub('\\$','',as.character(dollarAmt))),
         response = as.integer(response),
         workerid = as.factor(workerid))

samplemean <- function(x, d) {
  return(mean(x[d]))
}
ci = function(vals) {
  cis = boot.ci(boot(vals, samplemean, 1000), type="bca")
  low = cis$bca[1,4]
  high = cis$bca[1,5]
  return(data.frame(mean=mean(vals),
                    ci.low=low,
                    ci.high=high,
                    N=length(vals)))
}

aggr = d0 %>% group_by(item, dollarAmt, qType) %>%
#   summarise(response = mean(response)) %>%
  do(ci(.$response)) %>%
  rename(response=mean)
ggplot(aggr, aes(x=dollarAmt, y=response, colour=item, linetype=qType)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(x=dollarAmt, ymin=ci.low, ymax=ci.high), width=0) +
  facet_grid(~ item, scale='free') +
  scale_y_continuous(breaks = 1:9, limits=c(1, 9)) +
  theme_few() + scale_colour_few()
```

## Model fit