// time webppl S1_with_bins.wppl --require utils --fit_giveanumber 1

// Load index as last command line input.
var chain = last(process.argv);

var ignore_last_bin = true;

// var rating_type = "rating";
var rating_type = "normed_rating";

var totalIterations = (
  utils.get_variable("--iterations") ?
  utils.float(utils.get_variable("--iterations")) :
  50
);

var lag = (
  utils.get_variable("--lag") ?
  utils.float(utils.get_variable("--lag")) :
  1
);

var inductive_version = (
  utils.get_variable("--inductive_version") ?
  utils.get_variable("--inductive_version") :
  "inspect_joint"
);

var fit_giveanumber = utils.get_flag("--fit_giveanumber");

var fit_concrete = utils.get_flag("--fit_concrete");

var fit_inductive = utils.get_flag("--fit_inductive");

var fit_bins = utils.get_flag("--fit_bins");

var all_bins_at_once = utils.get_flag("--all_bins");
var all_sorites_at_once = utils.get_flag("--all_sorites");


var listener_level = 0;

display(fit_giveanumber ? "fitting to giveanumber" : "");
display(fit_concrete ? "fitting to concrete" : "");
display(fit_inductive ? "fitting to inductive" : "");
display(fit_bins ? "fitting to bins" : "");
display(all_sorites_at_once ? "model all_sorites_at_once" : "returning results from specific sorties expt");
display(all_bins_at_once ? "model all_bins_at_once" : "returning results from specific prior expt");
display("inductive version: " + inductive_version);

// Justine's give-a-number data.
var d_giveanumber = dataFrame(
  utils.readCSV("../data/priors/reformatted_give_a_number.csv").data
);

// This was the experiment in Noah's plots. It's the last one we did, it
// includes the relative clause phrasing.
var sorites_expt_id = "11";
// Sorites experiment data.
var d_sorites = _.filter(dataFrame(
  utils.readCSV("../data/sorites/sorites.csv").data
), all_sorites_at_once ? {} : {"id": sorites_expt_id});


// Focus on this experiment for fitting bins.
// This experiment used a pretty good range of values.
var bins_expt_id = "12";
var d_bins = _.filter(
  dataFrame(
    utils.readCSV("../data/priors/reformatted_bins.csv").data
  ),
  all_bins_at_once ? {} : {"exp": bins_expt_id}
);


// // 2 minutes for L0, fitting all bins and all sorites data
// var totalIterations = 50, lag =  1;

// // 13 minutes for L0, fitting all bins and all sorites data
// var totalIterations = 500, lag =  1;

// // 85 minutes for L0, fitting all bins and all sorites data
// // 10 minutes for L0, fitting bins (12), concrete (11), and inductive (11)
// // 10 minutes for L0, fitting bins (12) only
// var totalIterations = 5000, lag =  10;

// __ minutes for L0, fitting all bins and all sorites data
// 20 minutes for L0, fitting bins (12) only
// var totalIterations = 10000, lag = 20;

// var totalIterations = 50000, lag =  10;

// var totalIterations = 500000, lag =  10;
// var totalIterations = 500000, lag =  200;

var samples = totalIterations/lag, burn = totalIterations / 2;

var outfile = (
  'results-S1-'+
  'sigmax10_' +
  totalIterations+
  '_burn'+burn+
  '_lag'+lag+
  '_chain'+chain+
  ("_inductive_version_"+inductive_version) +
  (fit_concrete ? "_concrete" : "") +
  (fit_giveanumber ? "_giveanumber" : "") +
  (fit_inductive ? "_inductive" : "") +
  (fit_bins ? "_bins" : "") +
  (all_bins_at_once ? "_allbins" : ("_" + bins_expt_id)) +
  (all_sorites_at_once ? "_allsorites" : ("_" + sorites_expt_id)) +
  ("_listener" + listener_level) +
  "_" + rating_type + 
  (ignore_last_bin ? "_ignore_last_bin" : "") +
  '.csv'
);

display(outfile);

var eps = 0.00000000000001;
var logit = function(p) {
  return Math.log(p/(1 - p) + eps);
};

var remove_endpoints = function(x) {
  if (x<=0) {
    return 0.00001;
  } else if (x >= 1) {
    return 0.99999;
  } else {
    return x;
  }
}

// Unique objects from sorites experiments.
var objects = levels(d_sorites, "object");
var experiment_ids = levels(d_sorites, "id");

// Create bins for each item that coordinate well with the experiment design.
var bins = _.fromPairs(map(function(item){
  return [item, setup_bins(d_sorites, experiment_ids, item)]
}, objects));

var thetaPriors = _.fromPairs(map(function(item) {
  return [item, _.fromPairs(map(function(id) {
    return [id, Categorical({
      vs: bins[item][id]["theta"],
      ps: bins[item][id]["theta_prob"]
    })];
  }, experiment_ids.concat(["all"])))];
}, objects));

var bins_filename = "results/bins.json";
utils.writeJSON(bins, bins_filename);
display("bins file written to: " + bins_filename);


var utterancePrior = Infer({model: function(){
	return uniformDraw(["expensive", "silence"])
}});

var inductive_utterance_prior = Infer({model: function(){
  return uniformDraw(["inductive", "silence"])
}});

var meaning = function(utt,state, theta) {
  return utt=="expensive"? state > theta :
         utt=="expensive is false"? state<=theta :
         utt=='silence'? true :
         true
};

var model = function(){

  // Prior parameters
  var ALPHA1MAX = 10; // maximum rationality for speaker1
  // var ALPHA2MAX = 5; // maximum rationality for speaker2
  // var COSTMAX = 5; // maximum cost of the expensive utterance
  var MUMAX = 10; // maximum mu for lognormal price distributions
  var SIGMAX = 10; // maximum sigma for lognormal price distributions
  var SIGBINMAX = 0.5;//3; // for interpreting likert scale responses


  // Infer global params across all participants and all objects.
  // alpha1: uniformDrift({a: 0, b: ALPHA1MAX, width: 1}),
  var speakerOptimality = uniformDrift({a: 0, b: ALPHA1MAX, width: 1});
  query.add(["global_param", "speakerOptimality", "NA", "NA"], speakerOptimality);
  var sigbin = uniformDrift({a: 0, b: SIGBINMAX, width: 0.005}); // prob too small 10% is like prob good
  query.add(["global_param", "sigbin", "NA", "NA"], sigbin);
  // cost: uniformDrift({a: 0, b: COSTMAX, width: 0.5}),
  // standard deviation for bins responses

  // Iterate over all items in the sorites experiments.
  foreach(objects, function(item) {

    // Parameters for lognormal price distributions, different parameters for
    // each object.
    var item_params = {
      mu: uniformDrift({a: 0, b: MUMAX, width: 1}),
      sigma: uniformDrift({a: 0, b: SIGMAX, width: 0.5})
    };

    var item_bins = _.filter(d_bins, {"object": item});

    var item_giveanumber = _.filter(d_giveanumber, {"object": item});

    if (fit_giveanumber) {
      // Fit lognormal parameters to give-a-number data.
      // Factor by likelihood of sampled responses from give-a-number task.
      mapData(
        {data: item_giveanumber},
        function(d) {
          // Assume log of price is normally distributed, so use lognormal density
          // function.
          factor(utils.lognormalScore(d.price, item_params));
        }
      );
    }

    if (fit_bins) {
      var item_bins = _.filter(d_bins, {"object": item});

      // Fit lognormal parameters to bins data.
      // Factor by likelihood of likert responses per bin.
      mapData(
        {data: item_bins},
        function(d) {
          // display(d.UB);
          var upper = d.UB=="Inf" ? Infinity : utils.float(d.UB);
          var ignore_this_bin = (ignore_last_bin && upper == Infinity);
          if (!ignore_this_bin) {
            // display(upper);
            var lower = utils.float(d.LB);
            // display(upper);
            var prob = utils.lognormalCDF(upper, item_params) - utils.lognormalCDF(lower, item_params);
            // display(prob);

            var rating = utils.float(d[rating_type]);
            // display(normed_rating);
            factor(Gaussian({mu: prob, sigma: sigbin}).score(rating));
            // display(prob);
            // display(logit(prob));
            // observe(
            //   LogitNormal({mu: logit(prob), sigma: sigbin, a: 0-eps, b: 1+eps}),
            //   rating
            // );
          }
        }
      );
    }

    foreach(experiment_ids, function(expt_id) {

      // // State prior is lognormal, params inferred for each object.
      // var statePrior = Infer({model: function() {
      //   sample(DiscretizedLognormal(item_params, bins[item][expt_id]));
      // }});
      // var statePrior = DiscretizedLognormal(item_params, bins[item][expt_id]);
      var statePrior = DiscretizedLognormal(item_params, bins[item]["all"]);

      // Theta prob is computed at the outer level based on the bin widths.
      // var thetaPrior = thetaPriors[item][expt_id];
      var thetaPrior = thetaPriors[item]["all"];

    	/// RSA model
    	var make_listener0_with_theta = function(erp) {
        return function(utterance, theta) {
      	  Infer({model: function(){
      	    var state = sample(statePrior);
            // theta might be lifted
            //  var theta = theta ? theta : sample(thetaPrior);
              var theta = utils.isNumber(theta) ? theta : sample(thetaPrior);
      	    var m = meaning(utterance, state, theta);
      	    condition(m);
      	    return {state: state, theta: theta};
        }})};
      };
      var listener0_with_theta = cache(make_listener0_with_theta(statePrior), 10000);

      var make_listener0 = function(erp, joint_version) {
        return function(utterance, theta) {
          Infer({model: function() {
            var rs = sample(joint_version(utterance, theta));
            var state = rs.state;
            return state;
        }})
        };
      };
      var listener0 = cache(
        make_listener0(statePrior, listener0_with_theta),
        10000
      );

      var make_speaker = function(listener_fn) {
        return function(state, theta) {
          Infer({model: function(){
            var utterance = sample(utterancePrior);
            var L0 = listener_fn(utterance, theta);
            factor(speakerOptimality * L0.score(state))
            return utterance == "expensive" ? 1 : 0
          }})
        };
      };

      var speaker1 = make_speaker(listener0);

      var d_expt_obj = _.filter(d_sorites,
        {"object": item, "id": expt_id, "qtype": "concrete"});
      var dollar_amounts = levels(d_expt_obj, "dollar_amount");
      foreach(dollar_amounts, function(dollar_amount) {
        // var mid = bins[item][expt_id]["dollar_amount_lookups"][dollar_amount];
        var mid = bins[item]["all"]["dollar_amount_lookups"][dollar_amount];
        // TODO this should be the corresponding bin mid to this dollar_amount
        var endorsement = speaker1(mid);
        var endorsement_prob = Math.exp(endorsement.score(1));
        var likert_dist = Binomial({p: endorsement_prob, n: 9});
        // Iterate through responses.

        if (fit_concrete) {
          mapData(
            {data: _.filter(d_expt_obj, {"dollar_amount": dollar_amount})},
            function(d){
              observe(likert_dist, (utils.float(d.response)))
            }
          );
        }

        query.add(["S1", dollar_amount, expt_id, item], endorsement_prob);
      });

      var listener1 = cache(function(utterance) {
        Infer({model: function(){
          var state = sample(statePrior);
          var theta = sample(thetaPrior);
          var S1 = speaker1(state, theta);
          factor(utterance=="expensive" ? S1.score(1) : S1.score(0));
          return {"state": state, "theta": theta}
      }})}, 10000);

      // Expected probabiity that x-eps > theta given the L0 posterior on x and theta.
      var joint_inductive = function(epsilon, listener_level) {
        var listener_function = listener_level==0?listener0_with_theta:listener1;
        // listener_function("expensive");
        var listener_joint_distribution = listener_function("expensive");
        return expectation(listener_joint_distribution, function(interpretation_pair) {
          var x = interpretation_pair.state;
          // given the bin for theta and a particular x, what's the probability that x-epsilon>theta?
          var theta_mid = interpretation_pair.theta;
          // var theta_bin = bins[item][expt_id]["theta_lookups"][theta_mid];
          var theta_bin = bins[item]["all"]["theta_lookups"][theta_mid];
          var theta_lower = theta_bin.lower;
          var theta_upper = theta_bin.upper;
          var theta_prob = theta_upper-theta_lower;
          var new_price = x-epsilon;
          if (new_price < theta_lower) {
            return 0;
          } if (new_price > theta_upper) {
            return 1;
          } else {
            var inductive_prob = new_price - theta_lower;
            return inductive_prob/theta_prob;
          }
        });
      };

      // For each bin, we know L(x \in bin).
      // We want to shift this down eps to find L(x - eps \in bin).
      // This is the distribution over y that we're integrating over.
      // We can approximate this by adding eps/width_of_bin probability mass
      // from each bin (after the first) to the bin below.
      var shift_price_dist = function(old_dist, epsilon) {
        // the bins in question stay the same
        var bin_mids = old_dist.support();
        var old_probs_and_bin_mids = map(function(x) {
          return [Math.exp(old_dist.score(x)), x];
        }, bin_mids);
        var new_probs = reduce(function(x, collected_probs) {
          var bin_prob = x[0];
          var bin_mid = x[1];
          if (collected_probs.length == 0) {
            return [bin_prob]; // start with partial prob for 1st bin being the whole bin
          } else {
            var width = (bin_mid==0 ? 1 : bins[item]["all"]["width_lookups"][bin_mid]);

            var completed_probs = collected_probs.slice(0, collected_probs.length-1);

            var partial_prob_for_prev_bin = collected_probs[collected_probs.length-1];
            var extra_prob_for_prev_bin = bin_prob * (eps/width);
            var prev_bin_prob = partial_prob_for_prev_bin + extra_prob_for_prev_bin;

            var partial_prob_for_current_bin = bin_prob * (1-eps/width);
            return completed_probs.concat([prev_bin_prob, partial_prob_for_current_bin]);
          }
        }, [], _.reverse(old_probs_and_bin_mids.concat([[0, 0]])));

        var probs = new_probs.slice(0, new_probs.length-1);
        var vals = bin_mids;
        return Categorical({ps: probs, vs: vals});
      };

      var post_to_prior_inductive = function(epsilon, listener_level) {
        if (listener_level == 1) {
          display("pragmatic listener not implemented");
        }
        // Listener distribution gives us the probability that x falls into each
        // bin B given that the speaker says "expensive", averaging out theta.
        var expensive_prices_dist = listener0("expensive");
        // But what we want is the probability that x-eps falls into each bin B.
        // We can approximate this by adding eps/width_of_bin probability mass
        // from each bin (after the first) to the bin below.
        var less_expensive_prices_dist = shift_price_dist(
          expensive_prices_dist,
          epsilon
        );

        // We sample a bin, and we know the probability under L that Y's price
        // is in that bin. Then we just figure out the probability under S1 of
        // saying "expensive" given that the object in questions's price is in
        // that bin.
        return expectation(less_expensive_prices_dist, function(y) {
          var endorsement_for_y = speaker1(y).score(1);
          return endorsement_for_y;
        });
      };

      var communicate_dist_inductive = function(epsilon, listener_level) {
        if (listener_level == 1) {
          display("pragmatic listener not implemented");
        }
        // S1 knows the true params: item_params
        // inductive utterance:
        //   * x > theta AND
        //   * x - eps > theta
        // since one implies the other, the utterance is basically just
        //   x - eps > theta
        // where x ~ lognormal(mu, sig) and theta, mu, sig ~ unif
        var l0_given_inductive_premise = Infer({
          method: "rejection",
          samples: 100,
          model: function() {
            var a = 0;
            var b = bins[item]["all"]["max_theta"];
            var sampled_params = {
              mu: uniform({a: 0, b: MUMAX, width: 1}),
              sigma: uniform({a: 0, b: SIGMAX, width: 0.5})
            };
            var prob_inductive_true = (1 - (
              (1 / (b - a)) *
              (
                utils.lognormalCDF(b + epsilon, sampled_params) -
                utils.lognormalCDF(a + epsilon, sampled_params)
              )
            ));
            factor(Math.log(prob_inductive_true));
            return sampled_params;
          }
        });
        // display(l0_given_inductive_premise);
        // var l0_given_no_utterance = Infer({
        //   method: rejection
        //   model: function() {
        //     return {
        //       mu: uniformDrift({a: 0, b: MUMAX, width: 1}),
        //       sigma: uniformDrift({a: 0, b: SIGMAX, width: 0.5})
        //     };
        //   }
        // });
        var s1_for_dist_qud = Infer({
          method: "rejection",
          samples: 100,
          model: function() {
            var utterance = sample(inductive_utterance_prior);
            // var interpretation_dist = (
            //   (utterance == "inductive") ?
            //   l0_given_no_utterance :
            //   l0_given_inductive_premise
            // );
            // S1 knows the true params: item_params
            // so factor by how close the interpretation is to the truth!!!
            var listener_params = (
              (utterance=="inductive") ?
              (sample(l0_given_inductive_premise)) :
              {
                mu: uniformDrift({a: 0, b: MUMAX, width: 1}),
                sigma: uniformDrift({a: 0, b: SIGMAX, width: 0.5})
              }
            );
            factor(Gaussian({mu: item_params.mu, sigma: 1}).score(listener_params.mu));
            factor(Gaussian({mu: item_params.sigma, sigma: 1}).score(listener_params.sigma));
            return utterance;
          }
        });
        return s1_for_dist_qud.score("inductive");
      };

      var inductive = function(epsilon, listener_level) {
        if (inductive_version == "inspect_joint") {
          return joint_inductive(epsilon, listener_level);
        } else if (inductive_version == "post_to_prior") {
          return post_to_prior_inductive(epsilon, listener_level);
        } else if (inductive_version == "dist_qud") {
          return communicate_dist_inductive(epsilon, listener_level);
        } {
          display("error a203984asf");
        }
      };

      var ind_expt_obj = _.filter(d_sorites,
        {"object": item, "id": expt_id, "qtype": "inductive"});
      var epsilons = levels(ind_expt_obj, "dollar_amount");
      foreach(epsilons, function(dollar_amount) {
        var epsilon = utils.float(dollar_amount);
        var inductive_endorsement = inductive(epsilon, listener_level);
        var ind_likert_dist = Binomial({p: remove_endpoints(inductive_endorsement), n: 9});

        if (fit_inductive) {
          mapData(
            {data: _.filter(ind_expt_obj, {"dollar_amount": dollar_amount})},
            function(d){
              observe(ind_likert_dist, (utils.float(d.response)))
            }
          );
        }

        query.add(["Inductive", epsilon, expt_id, item], inductive_endorsement);

      });

    })

    query.add(["price_prior", "mu", "NA", item], item_params.mu);
    query.add(["price_prior", "sigma", "NA", item], item_params.sigma);
  });

	return query;
}

var posterior = Infer({
  model: model,
	method: "incrementalMH",
  samples: samples, burn: burn, lag: lag,
  verbose: T,
  verboseLag: totalIterations / 20
})

utils.writeQueryERP(posterior, "results/" + outfile, ["type", "param", "property", "category", "val"]);

"written to " + outfile;
