// ARGS.give_a_number data frame
// ARGS.concrete data frame
// ARGS.objects vector of objects

//////////////////////////////////////////
// helpers (move to package?)//////////
var foreach = function(lst, fn) {
    var foreach_ = function(i) {
        if (i < lst.length) {
            fn(lst[i]);
            foreach_(i + 1);
        }
    };
    foreach_(0);
};

var levels = function(df, label){
  return _.uniqBy(_.map(df, label));
};

var EPS = Number.EPSILON;

var objects = levels(ARGS.sorites, "object");

// TODO
// likert scale
// var likertParams = {LB: 0, UB: 9, binWidth: 1};
// var sampleLikertThreshold = function(b){
//  return b == likertParams.LB ? likertParams.LB + likertParams.binWidth/2 :
//         b == (likertParams.UB-1) ? likertParams.UB - likertParams.binWidth/2 :
//         gaussian(b + 0.5, 1/2)
// }
//

// Truth value function for possible utterances.
var meaning = function(utt, state, theta) {
  return (utt == "expensive" ? state > theta :
          (utt == "not expensive" ? state <= theta :
           (utt == "cheap" ? state < theta :
            (utt == "silence" ? true : true))));
};

// Construct a single row in the output dataframe.
var add_output_datapoint = function(key, val) {
  var basic_key = {
    type: "NA",
    label: "NA",
    dollar_amount: "NA",
    object: "NA",
    mid: "NA",
    UB: "NA",
    LB: "NA"
  };
  var new_key = _.extend(basic_key, key);
  var keystring = JSON.stringify(new_key);
  query.add(keystring, val);
};


var give_a_number_model = function() {
  var MUMAX = 10;
  var SIGMAX = 3;

  foreach(objects, function(obj) {

    // subset each data type
    // var prior_bins = _.filter(ARGS.priorbins, {item: obj});
    var give_a_number = _.filter(ARGS.give_a_number, {object: obj});

    // var concrete = _.filter(ARGS.sorites, {object: obj, qtype: "concrete"});
    // var inductive = _.filter(ARGS.sorites, {object: obj, qtype: "inductive"});

    // priors on parameters of RSA state priors
    var item_params = {
      mu: uniformDrift({a: 0, b: MUMAX, width: 1}),
      sigma: uniformDrift({a: 0, b: SIGMAX, width: 0.5})
    };

    // factor by likelihood of sampled responses from give-a-number task
    mapData({data: give_a_number}, function(d) {
      // assume log of price is normally distributed
      var prob_of_sampled_price = Gaussian(item_params).score(
        Math.log(d.price)
      );
      factor(prob_of_sampled_price);
    });

    add_output_datapoint(
      {type: "prior", object: obj, label: "mu"},
      item_params.mu
    );
    add_output_datapoint(
      {type: "prior", object: obj, label: "sigma"},
      item_params.sigma
    );

  });

  return query;
};

var get_score = function(erp, value) {
  // TODO
  // interpolation between number bigger and number smaller
  var support = erp.support();
  var below = utils.closest_value(support, value, "below");
  var above = utils.closest_value(support, value, "above");
  // TODO
  var below_score = erp.score(below);
  var above_score = erp.score(above);
  return (below_score + above_score) / 2;
};

var concrete_premise_model = function() {
  var ALPHA1MAX = 20;
  var ALPHA2MAX = 5;
  var COSTMAX = 10;
  var MUMAX = 10;
  var SIGMAX = 3;
  var SIGBINMAX = 0.5;

  var global_params = {
    alpha1: uniformDrift({a: 0, b: ALPHA1MAX, width: 1}),
    alpha2: uniformDrift({a: 0, b: ALPHA2MAX, width: 0.5}),
    cost: uniformDrift({a: 0, b: COSTMAX, width: 0.5}),
    sigbin: uniformDrift({a: 0, b: SIGBINMAX, width: 0.5})
    // TO FIX
    // likert_thresholds: map(sampleLikertThreshold,
    //  _.range(likertParams.LB, likertParams.UB, likertParams.binWidth)
    // )
  };

  var adjectivePriorProb = Math.exp(-global_params.cost);
  var utterancePrior = Categorical({
    vs: ["expensive", "silence"],
    ps: [adjectivePriorProb, 1]
  });

  foreach(objects, function(obj) {

    // subset each data type
    var give_a_number = _.filter(ARGS.give_a_number, {object: obj});

    var concrete = _.filter(ARGS.sorites, {object: obj, qtype: "concrete"});

    // priors on parameters of RSA state priors
    var item_params = {
      mu: uniformDrift({a: 0, b: MUMAX, width: 1}),
      sigma: uniformDrift({a: 0, b: SIGMAX, width: 0.5})
    };

    // factor by likelihood of sampled responses from give-a-number task
    mapData({data: give_a_number}, function(d) {
      // assume log of price is normally distributed
      var prob_of_sampled_price = Gaussian(item_params).score(
        Math.log(d.price)
      );
      factor(prob_of_sampled_price);
    });

    add_output_datapoint(
      {type: "prior", object: obj, label: "mu"},
      item_params.mu
    );
    add_output_datapoint(
      {type: "prior", object: obj, label: "sigma"},
      item_params.sigma
    );


    var dollar_amounts = levels(concrete, "dollar_amount");
    var max_price = utils.max(dollar_amounts);

   /// RSA model
   var listener0 = function(utterance, theta) {
     Infer({
      method: "incrementalMH",
      samples: ARGS.internal_iterations,
      lag: 1,
      burn: ARGS.internal_iterations/2,
      model: function(){
        var log_price = gaussian(item_params);
        var price = Math.exp(log_price);
        var state = price;
         var m = meaning(utterance, state, theta);
         condition(m);
         return state;
      }})};

   var speaker1 = function(state, theta) {
     Infer({
      method: "incrementalMH",
      samples: ARGS.internal_iterations,
      lag: 1,
      burn: ARGS.internal_iterations/2,
      model: function(){
         var utterance = sample(utterancePrior);
         var L0 = listener0(utterance, theta);
         // look for the closest item in the support and get the score of that.
         // actually, maybe we can just rescale lognormal somehow.
         factor(global_params.alpha1 * get_score(L0, state));
         return utterance;
       }})};

     // no continuous parameters, so caching makes sense.
   var listener1 = cache(function(utterance) {
     Infer({
      method: "incrementalMH",
      samples: ARGS.internal_iterations,
      lag: 1,
      burn: ARGS.internal_iterations/2,
      verbose: true,
      verboseLag: ARGS.internal_iterations / 10,
      model: function(){
        var log_price = gaussian(item_params);
        var price = Math.exp(log_price);
        var state = price;
        var theta = uniform({a: 0, b: max_price});
         var S1 = speaker1(state, theta);
         observe(S1, utterance);
         return state;
       }})});

    var cached_get_score = cache(get_score);
   var speaker2 = function(speaker_belief_dollar_amount){
     Infer({
      method: "enumerate",
      model: function(){
         var utterance = sample(utterancePrior);
         var L1 = listener1(utterance);
         factor(global_params.alpha2 * cached_get_score(L1,speaker_belief_dollar_amount));
         return utterance;
      }})};

    foreach(dollar_amounts, function(dollar_amount){
      var concrete_byDollar = _.filter(concrete, {dollar_amount: dollar_amount});

      var concretePrediction = speaker2(dollar_amount);

      mapData({data: concrete_byDollar}, function(d){
        // for now, assume if > 5, then "YES", other NO
        var binarizedResponse = (d.response > 5) ? "expensive" : "silence";
        observe(concretePrediction, binarizedResponse);
      });

      var concrete_prob = Math.exp(concretePrediction.score("expensive"));
      add_output_datapoint(
        {type: "concrete", object: obj, dollar_amount: JSON.stringify(dollar_amount)},
        concrete_prob
      );

    });

  });

  add_output_datapoint(
    {type: "param", label: "alpha1"},
    global_params.alpha1
  );
  add_output_datapoint(
    {type: "param", label: "alpha2"},
    global_params.alpha2
  );
  add_output_datapoint(
    {type: "param", label: "cost"},
    global_params.cost
  );

  return query;
};

var model = function() {
  var ALPHA1MAX = 20;
  var ALPHA2MAX = 5;
  var COSTMAX = 10;
  var MUMAX = 10;
  var SIGMAX = 3;
  var SIGBINMAX = 0.5;

  // var global_params = {
  //   alpha1: uniformDrift({a: 0, b: ALPHA1MAX, width: 1}),
  //   alpha2: uniformDrift({a: 0, b: ALPHA2MAX, width: 0.5}),
  //   cost: uniformDrift({a: 0, b: COSTMAX, width: 0.5}),
  //   sigbin: uniformDrift({a: 0, b: SIGBINMAX, width: 0.5})
  //   // TO FIX
  //   // likert_thresholds: map(sampleLikertThreshold,
  //   //  _.range(likertParams.LB, likertParams.UB, likertParams.binWidth)
  //   // )
  // };

  // var adjectivePriorProb = Math.exp(-global_params.cost);
  // var utterancePrior = Categorical({
  //   vs: ["expensive", "silence"],
  //   ps: [adjectivePriorProb, 1]
  // });

  foreach(objects, function(obj) {

    // subset each data type
    // var prior_bins = _.filter(ARGS.priorbins, {item: obj});
    var give_a_number = _.filter(ARGS.give_a_number, {object: obj});

    // var concrete = _.filter(ARGS.sorites, {object: obj, qtype: "concrete"});
    // var inductive = _.filter(ARGS.sorites, {object: obj, qtype: "inductive"});

    // priors on parameters of RSA state priors
    var item_params = {
      mu: uniformDrift({a: 0, b: MUMAX, width: 1}),
      sigma: uniformDrift({a: 0, b: SIGMAX, width: 0.5})
    };

    // factor by likelihood of sampled responses from give-a-number task
    mapData({data: give_a_number}, function(d) {
      // assume log of price is normally distributed
      var prob_of_sampled_price = Gaussian(item_params).score(
        Math.log(d.price)
      );
      factor(prob_of_sampled_price);
    });

    // factor based on binned histogram data
    // mapData({data: prior_bins}, function(d) {
    //   var ub = d.UB;
    //   var lb = d.LB;
    //   var human_response = d.normed_rating;
    //   var actual_prob = lognormal_prob_bin(item_params, lb, ub);
    //   factor(Gaussian(
    //     {mu: actual_prob, sigma: global_params.sigbin}
    //   ).score(human_response));
    // });


    // var posterior_predictive_itemPrior = Math.exp(gaussian(item_params));
    // add_output_datapoint(
    //   {type: "prior", object: obj, label: "price"},
    //   posterior_predictive_itemPrior
    // );

    add_output_datapoint(
      {type: "prior", object: obj, label: "mu"},
      item_params.mu
    );
    add_output_datapoint(
      {type: "prior", object: obj, label: "sigma"},
      item_params.sigma
    );

  //   var posterior_predictive_itemPrior = Math.exp(gaussian(item_params));
  //   add_output_datapoint(
  //     {type: "prior", object: obj, label: "price"},
  //     posterior_predictive_itemPrior
  //   );

  //   var bins = get_bins(obj);
  //   var statePrior = DiscretizedLognormal(item_params, bins);
  //   var thetaPrior = DiscretizedUniform(bins);

  //   // get posterior predtive for prior ratings for bins
  //   foreach(bins, function(bin) {
  //     var ub = bin.UB;
  //     var lb = bin.LB;
  //     var actual_prob = lognormal_prob_bin(item_params, lb, ub);
  //     var posterior_predictive_bin_rating = gaussian(
  //       {mu: actual_prob, sigma: global_params.sigbin}
  //     );
  //     add_output_datapoint(
  //       {
  //         type: "prior",
  //         object: obj,
  //         label: "bins",
  //         mid: JSON.stringify(bin.mid),
  //         UB: JSON.stringify(ub),
  //         LB: JSON.stringify(lb)
  //       },
  //       posterior_predictive_bin_rating
  //     );
  //   });

  //  /// RSA model
  //  var listener0 = cache(function(utterance, theta) {
  //    Infer({model: function(){
  //      var state = sample(statePrior)
  //      var m = meaning(utterance, state, theta)
  //      condition(m)
  //      return state
  //   }})}, 10000)

  //  var speaker1 = cache(function(state, theta) {
  //    Infer({model: function(){
  //      var utterance = sample(utterancePrior);
  //      var L0 = listener0(utterance, theta);
  //      factor(global_params.alpha1 * L0.score(state))
  //      return utterance
  //    }})}, 10000)

  //  var listener1 = cache(function(utterance) {
  //    Infer({model: function(){
  //      var state = sample(statePrior);
  //      var theta = sample(thetaPrior);
  //      var S1 = speaker1(state, theta)
  //      observe(S1, utterance)
  //      return state
  //    }})}, 10000)

  //  var speaker2 = function(speakerBeliefs){
  //    Infer({model: function(){
  //      var utterance = sample(utterancePrior);
  //      var L1 = listener1(utterance)
  //      factor(global_params.alpha2 * L1.score(speakerBeliefs))
  //      return utterance
  //   }})}

  //   var dollar_amounts = levels(concrete, "dollar_amount")

  //   // foreach(dollar_amounts, function(dollar_amount){
  //   //   var concrete_byDollar = _.filter(concrete, {dollar_amount: dollar_amount});
  //   //   var inductive_byDollar = _.filter(inductive, {dollar_amount: dollar_amount});

  //   //   var discretized_dollar_amount = convert_to_bin(dollar_amount, bins);

  //   //   var concretePrediction = speaker2(discretized_dollar_amount);

  //   //   mapData({data: concrete_byDollar}, function(d){
  //   //     // for now, assume if > 5, then "YES", other NO
  //   //     var binarizedResponse = (d.response > 5) ? "expensive" : "silence";
  //   //     observe(concretePrediction, binarizedResponse)
  //   //   })

  //   //   var concrete_prob = Math.exp(concretePrediction.score("expensive"));
  //   //   add_output_datapoint(
  //   //     {type: "concrete", object: obj, dollar_amount: JSON.stringify(dollar_amount)},
  //   //     concrete_prob
  //   //   );

  //   // })

  });

  // add_output_datapoint(
  //   {type: "param", label: "alpha1"},
  //   global_params.alpha1
  // );
  // add_output_datapoint(
  //   {type: "param", label: "alpha2"},
  //   global_params.alpha2
  // );
  // add_output_datapoint(
  //   {type: "param", label: "cost"},
  //   global_params.cost
  // );
  // add_output_datapoint(
  //   {type: "param", label: "sigbin"},
  //   global_params.sigbin
  // );

  return query;
};


"finished";
