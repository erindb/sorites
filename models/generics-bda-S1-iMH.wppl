// time webppl generics-bda-L0-S1-iMH.wppl --require utils 1

var chain = last(process.argv) // load index as last command line index

// penultimate argument is the semantics
// uncertain = uncertain threshold
// fixed = fixed threshold at lowest threshold value
// var semantics = process.argv[process.argv.length - 2]
// console.log(semantics)
var priorFilePrefix = "prior-3",
    // priorFilePrefix = "prior-manipulation-3",
    interpretationFilePrefix = "interpretation-6",
    endorsementFilePrefix = "endorsement-1";

// expt 2: measured priors
var d_prior = readDataFile("prior", priorFilePrefix),
    d_prior_catch = readCatchFile("prior", priorFilePrefix),
    d_prior_subj = readSubjFile("prior", priorFilePrefix),
    d_interpretation = readDataFile("interpretation", interpretationFilePrefix),
    d_interpretation_catch = readCatchFile("interpretation", interpretationFilePrefix),
    d_interpretation_subj = readSubjFile("interpretation", interpretationFilePrefix),
    d_endorsement = readDataFile("endorsement", endorsementFilePrefix),
    d_endorsement_catch = readCatchFile("endorsement", endorsementFilePrefix),
    d_endorsement_subj = readSubjFile("endorsement", endorsementFilePrefix);

// expt 3: Prior manipulation
// var d_prior = readDataFile("prior-manipulation", priorFilePrefix),
//     d_prior_catch = readCatchFile("prior-manipulation", priorFilePrefix),
//     d_prior_subj = readSubjFile("prior-manipulation", priorFilePrefix),
//     d_interpretation = readDataFile("prior-manipulation", interpretationFilePrefix),
//     d_interpretation_catch = readCatchFile("prior-manipulation", interpretationFilePrefix),
//     d_interpretation_subj = readSubjFile("prior-manipulation", interpretationFilePrefix);

var priorWorkeridsPassed = passCatchTrials(d_prior_catch),
    interpretationWorkeridsPassed = passCatchTrials(d_interpretation_catch),
    endorsementWorkeridsPassed = passCatchTrials(d_endorsement_catch),
    priorWorkeridsEnglish = nativeEnglish(d_prior_subj),
    interpretationWorkeridsEnglish = nativeEnglish(d_interpretation_subj),
    endorsementWorkeridsEnglish = nativeEnglish(d_endorsement_subj);

var data = {
	prior: filter(function(di){
    return ((priorWorkeridsPassed.indexOf(di.workerid) > -1) && 
      (priorWorkeridsEnglish.indexOf(di.workerid) > -1) && 
      utils.isNumber(di.response) &&
      di.trial_type == "prevalence_elicitation")
  }, map(function(d){ return extend(d, {
		avoided_endval: avoidEnds(d.response)
  })}, d_prior)),
	interpretation: filter(function(di){
    return ((interpretationWorkeridsPassed.indexOf(di.workerid) > -1) && 
      (interpretationWorkeridsEnglish.indexOf(di.workerid) > -1) && 
      utils.isNumber(di.response) &&
      di.trial_type == "implied_prevalence")
  }, map(function(d){ return extend(d, {
		binnedResponse:  utils.closest( midBins.slice(1), d.response)
	})}, d_interpretation)),
  endorsement: filter(function(di){
    return ((endorsementWorkeridsPassed.indexOf(di.workerid) > -1) && 
      (endorsementWorkeridsEnglish.indexOf(di.workerid) > -1) && 
      utils.isNumber(di.response) &&
      di.trial_type == "truth_conditions")
  }, map(function(d){ return extend(d, {
    binned_prevalence:  utils.closest( midBins, d.prevalence_level / 100)
  })}, d_endorsement))
};

var utterancePrior = Infer({model: function(){
	return uniformDraw(["expensive", "silence"])
}});

var meaning = function(utt,state, theta) {
  return utt=="expensive"? state > theta :
         utt=="expensive is false"? state<=theta :
         utt=='silence'? true :
         true
}
//
var properties = levels(data.interpretation, "property");
var prevalence_levels = levels(data.endorsement, "binned_prevalence")

var nullParams = {a:1, b:100}, nullDistribution = Beta(nullParams);
// properties

// var addNoise = function(dist, noise){
//   return Infer({model: function(){
//     return flip(noise) ? uniformDraw(midBins) : sample(dist)
//   }})
// }

// var fixedThreshold = (semantics == "some") ?_.min( thetaBins) :
//   (semantics == "most") ? 0.5 :
//   false

var model = function(){

  // Prior parameters
  var ALPHA1MAX = 10; // maximum rationality for speaker1
  // var ALPHA2MAX = 5; // maximum rationality for speaker2
  // var COSTMAX = 5; // maximum cost of the expensive utterance
  var MUMAX = 10; // maximum mu for lognormal price distributions
  var SIGMAX = 3; // maximum sigma for lognormal price distributions
  // var SIGBINMAX = 0.5; // for interpreting likert scale responses

  // Infer global params across all participants and all objects.
  // var global_params = {
    // // rationality for speaker1
    // alpha1: uniformDrift({a: 0, b: ALPHA1MAX, width: 1}),
  var speakerOptimality = uniformDrift({a: 0, b: ALPHA1MAX, width: 1});
    // // cost of expensive utterance
    // cost: uniformDrift({a: 0, b: COSTMAX, width: 0.5}),
    // standard deviation for bins responses
    // sigbin: uniformDrift({a: 0, b: SIGBINMAX, width: 0.5})
  // };

  // TODO: read this from data
  var objects = ["laptop"];

  foreach(objects, function(item) {

    // Parameters for lognormal price distributions, different parameters for
    // each object.
    var item_params = {
      mu: uniformDrift({a: 0, b: MUMAX, width: 1}),
      sigma: uniformDrift({a: 0, b: SIGMAX, width: 0.5})
    };

    // State prior is lognormal, params inferred for each object.
    var statePrior = Infer({model: function() {
      sample(DiscretizedLognormal(item_params))
    }});

   //  var propertyData = {
   //      prior: _.filter(data.prior, {property: item}),
   //      interpretation: _.filter(data.interpretation, {property: item}),
   //      endorsement: _.filter(data.endorsement, {property: item})
   //  };

   //  var weights = normalize(repeat(numberOfComponents,
   //    function(){ return uniformDrift({a: 0, b: 1, width:0.2}) }
   //  ))

  	// mapData({data: propertyData.prior}, function(d){
   //    var componentLogLikelihood = map2(function(w, params){
   //      Math.log(w) + Beta(params).score(d.avoided_endval)
   //    }, weights, componentParameters)
   //    var scr = util.logsumexp(componentLogLikelihood)
   //    scr == -Infinity ? displayObj(d) : null
  	// 	factor(scr)
  	// })

   //  foreach(_.range(numberOfComponents), function(i){

   //    query.add(["componentParameters", item, i, "weight"],
   //      weights[i]
   //    )
   //    query.add(["componentParameters", item, i, "alpha"],
   //      componentParameters[i]["a"]
   //    )

   //    query.add(["componentParameters", item, i, "beta"],
   //      componentParameters[i]["b"]
   //    )

   //  })

  	// var statePrior = Infer({model: function(){
   //    sample(DiscretizedBeta(componentParameters[discrete(weights)]))
  	// }});

  	/// RSA model
  	var listener0 = cache(function(utterance) {
  	  Infer({model: function(){
  	    var state = sample(statePrior);
        var theta = sample(thetaPrior);
  	    var m = meaning(utterance, state, theta)
  	    condition(m)
  	    return state
  	 }})}, 10000)

  var interpretationPrediction = listener0("expensive")

   // mapData({data:propertyData.interpretation}, function(d){
   //   // display(d)
   //   var scr = interpretationPrediction.score(d.binnedResponse)
   //   scr == -Infinity ? displayObj(d) : null
   //   // display(scr)
   //   observe(interpretationPrediction, d.binnedResponse)
   // })

   // var supp = interpretationPrediction.support()

   // foreach(supp, function(s){
   //   query.add(["prediction", item, "postDist", s], Math.exp(interpretationPrediction.score(s)))
   // })

   var speaker1 = function(state) {
    Infer({model: function(){
      var utterance = sample(utterancePrior);
      var L0 = listener0(utterance);
      factor(speakerOptimality * L0.score(state))
      return utterance == "expensive" ? 1 : 0
    }})}


   foreach(prevalence_levels, function(p){

     // var endorsementData = _.filter(propertyData.endorsement, {binned_prevalence: p})
     var endorsement = speaker1(p)

     // mapData({data:endorsementData}, function(d){
     //   // display(d)
     //   var scr = endorsement.score(d.response)
     //   scr == -Infinity ? displayObj(d) : null
     //   // display(scr)
     //   observe(endorsement, d.response)
     // })


     query.add(["endorsement", item, -99, p], Math.exp(endorsement.score(1)))

   })


   query.add(["prediction", item, -99, "prior"], expectation(statePrior))
   query.add(["prediction", item, -99, "posterior"], expectation(interpretationPrediction))
 })

   // RECORD PARAMETERS AND PREDICTIVES
   query.add(["speakerOptimality", -99, -99, -99], speakerOptimality)
   // semantics == "most" ? query.add(["noise", -99, -99, -99], noise) : null

	return query
}


// data.endorsement
var totalIterations = 500, lag =  1;
// var totalIterations = 500000, lag =  200;
var samples = totalIterations/lag, burn = totalIterations / 2;

var outfile = 'results-genint-S1-endorsePrediction-int6-prior3-3Components_'+totalIterations+'_burn'+burn+'_lag'+lag+'_chain'+chain+'.csv'
// var outfile = 'results-asymmetry-L0-S1-int6-end1-prior3n200-3Components_'+semantics+'-semantics_'+ totalIterations+'_burn'+burn+'_lag'+lag+'_chain'+chain+'.csv'

var posterior = Infer({
  model: model,
	method: "incrementalMH",
  samples: samples, burn: burn, lag: lag,
  verbose: T,
  verboseLag: totalIterations / 20
})

utils.writeQueryERP(posterior, "results/" + outfile, ["type", "param", "property", "category", "val"]);

"written to " + outfile;