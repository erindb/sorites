// time webppl S1.wppl --require utils --fit_giveanumber 1

// Load index as last command line input.
var chain = last(process.argv);

var fit_giveanumber = utils.get_flag("--fit_giveanumber");

var fit_concrete = utils.get_flag("--fit_concrete");

var fit_inductive = utils.get_flag("--fit_inductive");

var all_expts_at_once = utils.get_flag("--all");

display(fit_giveanumber ? "fitting to giveanumber" : "");
display(fit_concrete ? "fitting to concrete" : "");
display(fit_inductive ? "fitting to inductive" : "");
display(all_expts_at_once ? "fitting to all_expts_at_once" : "ERROR!!! NOT IMPLEMENTED ATM");

// Justine's give-a-number data.
var d_giveanumber = dataFrame(
  utils.readCSV("../data/priors/reformatted_give_a_number.csv").data
);

var listener_level = 0;


// // For 07c, takes 5s.
// // Computing a joint dist takes 5s
// // 
// var totalIterations = 50, lag =  1;
// // var totalIterations = 500, lag =  1;

// For *just* fitting lognormal params to give a number data:
//   - Takes 8s
//   - Fits give a number data well
// Adding setup of discrete priors and listener0, it takes 10s.
// (Actually computing listener0 takes 13s.)
// Setting up every function up to S1 takes 12s.
// Adding dollar_amounts and an empty foreach takes 12s.
// Actually running S1 to get endorsements takes 28s.
// And observing the endorsement with mapData takes 1m.
// And recording endorsements takes about 1m.
// Adding likert takes 1m 30s.
// Doubling nbins takes ~2m and fits much better, and doubling nbins again:
//   - takes 2m 30s
//   - fits concrete premise well, but not perfectly.
//   - Doesn't fit priors as well when we fit the concrete premise.
//   - adding iterations does not help.
//   - btw, 8x takes __ and fits __.
// Alternatively, iterating through all experiments
//   - takes 8m 30s
//   - fits terribly
// Alternatively, computing a joint dist:
//   - by itself takes takes 1m 30s.
var totalIterations = 5000, lag =  10;

// var totalIterations = 10000, lag = 20;

// // For *just* fitting lognormal params to give a number data, takes 40s.
// // For the whole shebang with one experiment, takes ~2m, 30s.
// var totalIterations = 50000, lag =  10;

// // 71m
// var totalIterations = 500000, lag =  10;
// // var totalIterations = 500000, lag =  200;

var samples = totalIterations/lag, burn = totalIterations / 2;

var outfile = (
  'results-S1-'+
  totalIterations+
  '_burn'+burn+
  '_lag'+lag+
  '_chain'+chain+
  (fit_concrete ? "_concrete" : "") +
  (fit_giveanumber ? "_giveanumber" : "") +
  (fit_inductive ? "_inductive" : "") +
  (all ? "_all" : "") +
  ("_listener" + listener_level) +
  '.csv'
);

display(outfile);

var remove_endpoints = function(x) {
  if (x<=0) {
    return 0.00001;
  } else if (x >= 1) {
    return 0.99999;
  } else {
    return x;
  }
}

// // Focus on this experiment for fitting first.
// // This experiment used a pretty good range of values.
// var expt_id = "07c";

// Sorites experiment data.
var d_sorites = _.filter(dataFrame(
  utils.readCSV("../data/sorites/sorites.csv").data
));
// Unique objects from sorites experiments.
var objects = levels(d_sorites, "object");
var experiment_ids = levels(d_sorites, "id");

// Create bins for each item that coordinate well with the experiment design.
var bins = _.fromPairs(map(function(item){
  return [item, setup_bins(d_sorites, experiment_ids, item)]
}, objects));

var thetaPriors = _.fromPairs(map(function(item) {
  return [item, _.fromPairs(map(function(id) {
    return [id, Categorical({
      vs: bins[item][id]["theta"],
      ps: bins[item][id]["theta_prob"]
    })];
  }, experiment_ids.concat(["all"])))];
}, objects));

var bins_filename = "results/bins.json";
utils.writeJSON(bins, bins_filename);
display("bins file written to: " + bins_filename);


var utterancePrior = Infer({model: function(){
	return uniformDraw(["expensive", "silence"])
}});

var meaning = function(utt,state, theta) {
  return utt=="expensive"? state > theta :
         utt=="expensive is false"? state<=theta :
         utt=='silence'? true :
         true
};

var model = function(){

  // Prior parameters
  var ALPHA1MAX = 10; // maximum rationality for speaker1
  // var ALPHA2MAX = 5; // maximum rationality for speaker2
  // var COSTMAX = 5; // maximum cost of the expensive utterance
  var MUMAX = 10; // maximum mu for lognormal price distributions
  var SIGMAX = 3; // maximum sigma for lognormal price distributions
  // var SIGBINMAX = 0.5; // for interpreting likert scale responses

  // Infer global params across all participants and all objects.
  // var global_params = {
    // // rationality for speaker1
    // alpha1: uniformDrift({a: 0, b: ALPHA1MAX, width: 1}),
  var speakerOptimality = uniformDrift({a: 0, b: ALPHA1MAX, width: 1});
    // // cost of expensive utterance
    // cost: uniformDrift({a: 0, b: COSTMAX, width: 0.5}),
    // standard deviation for bins responses
    // sigbin: uniformDrift({a: 0, b: SIGBINMAX, width: 0.5})
  // var sigbin = uniformDrift({a: 0, b: SIGBINMAX, width: 0.5});
  // };

  // Iterate over all items in the sorites experiments.
  foreach(objects, function(item) {

    // Parameters for lognormal price distributions, different parameters for
    // each object.
    var item_params = {
      mu: uniformDrift({a: 0, b: MUMAX, width: 1}),
      sigma: uniformDrift({a: 0, b: SIGMAX, width: 0.5})
    };

    var item_giveanumber = _.filter(d_giveanumber, {"object": item});

    if (fit_giveanumber) {
      // Fit lognormal parameters to give-a-number data.
      // Factor by likelihood of sampled responses from give-a-number task.
      mapData(
        {data: item_giveanumber},
        function(d) {
          // Assume log of price is normally distributed, so use lognormal density
          // function.
          factor(utils.lognormalScore(d.price, item_params));
        }
      );
    }

    foreach(experiment_ids, function(expt_id) {

      // // State prior is lognormal, params inferred for each object.
      // var statePrior = Infer({model: function() {
      //   sample(DiscretizedLognormal(item_params, bins[item][expt_id]));
      // }});
      // var statePrior = DiscretizedLognormal(item_params, bins[item][expt_id]);
      var statePrior = DiscretizedLognormal(item_params, bins[item]["all"]);

      // Theta prob is computed at the outer level based on the bin widths.
      // var thetaPrior = thetaPriors[item][expt_id];
      var thetaPrior = thetaPriors[item]["all"];

    	/// RSA model
    	var listener0_with_theta = cache(function(utterance, theta) {
    	  Infer({model: function(){
    	    var state = sample(statePrior);
          // theta might be lifted
          var theta = theta ? theta : sample(thetaPrior);
    	    var m = meaning(utterance, state, theta);
    	    condition(m);
    	    return {state: state, theta: theta};
      }})}, 10000);

      var listener0 = cache(function(utterance, theta) {
        Infer({model: function() {
          var rs = sample(listener0_with_theta(utterance, theta));
          var state = rs.state;
          return state;
      }})}, 10000)

      var speaker1 = function(state, theta) {
        Infer({model: function(){
          var utterance = sample(utterancePrior);
          var L0 = listener0(utterance, theta);
          factor(speakerOptimality * L0.score(state))
          return utterance == "expensive" ? 1 : 0
        }})
      };

      var d_expt_obj = _.filter(d_sorites,
        {"object": item, "id": expt_id, "qtype": "concrete"});
      var dollar_amounts = levels(d_expt_obj, "dollar_amount");
      foreach(dollar_amounts, function(dollar_amount) {
        // var mid = bins[item][expt_id]["dollar_amount_lookups"][dollar_amount];
        var mid = bins[item]["all"]["dollar_amount_lookups"][dollar_amount];
        // TODO this should be the corresponding bin mid to this dollar_amount
        var endorsement = speaker1(mid);
        var endorsement_prob = Math.exp(endorsement.score(1));
        var likert_dist = Binomial({p: endorsement_prob, n: 9});
        // Iterate through responses.

        if (fit_concrete) {
          mapData(
            {data: _.filter(d_expt_obj, {"dollar_amount": dollar_amount})},
            function(d){
              observe(likert_dist, (utils.float(d.response)))
            }
          );
        }

        query.add(["S1", dollar_amount, expt_id, item], endorsement_prob);
      });

      var listener1 = cache(function(utterance) {
        Infer({model: function(){
          var state = sample(statePrior);
          var theta = sample(thetaPrior);
          var S1 = speaker1(state, theta);
          factor(utterance=="expensive" ? S1.score(1) : S1.score(0));
          return {"state": state, "theta": theta}
      }})}, 10000);

      // Expected probabiity that x-eps > theta given the L0 posterior on x and theta.
      var inductive = function(epsilon, listener_level) {
        var listener_function = listener_level==0?listener0_with_theta:listener1;
        listener_function("expensive");
        return expectation(listener_function("expensive"), function(interpretation) {
          var x = interpretation.state;
          // given the bin for theta and a particular x, what's the probability that x-epsilon>theta?
          var theta_mid = interpretation.theta;
          // var theta_bin = bins[item][expt_id]["theta_lookups"][theta_mid];
          var theta_bin = bins[item]["all"]["theta_lookups"][theta_mid];
          var theta_lower = theta_bin.lower;
          var theta_upper = theta_bin.upper;
          var theta_prob = theta_upper-theta_lower;
          var new_price = x-epsilon;
          if (new_price < theta_lower) {
            return 0;
          } if (new_price > theta_upper) {
            return 1;
          } else {
            var inductive_prob = new_price - theta_lower;
            return inductive_prob/theta_prob;
          }
        });
      };

      var ind_expt_obj = _.filter(d_sorites,
        {"object": item, "id": expt_id, "qtype": "inductive"});
      var epsilons = levels(ind_expt_obj, "dollar_amount");
      foreach(epsilons, function(dollar_amount) {
        var epsilon = utils.float(dollar_amount);
        var inductive_endorsement = inductive(epsilon, listener_level);
        var ind_likert_dist = Binomial({p: remove_endpoints(inductive_endorsement), n: 9});

        if (fit_inductive) {
          mapData(
            {data: _.filter(ind_expt_obj, {"dollar_amount": dollar_amount})},
            function(d){
              observe(ind_likert_dist, (utils.float(d.response)))
            }
          );
        }

        query.add(["Inductive", epsilon, expt_id, item], inductive_endorsement);

      });

    })

    query.add(["price_prior", "mu", "NA", item], item_params.mu);
    query.add(["price_prior", "sigma", "NA", item], item_params.sigma);
  });

  // RECORD PARAMETERS
  query.add(["global_param", "speakerOptimality", "NA", "NA"], speakerOptimality);
  // query.add(["param", "sigbin", "global", "NA"], sigbin)

	return query;
}

var posterior = Infer({
  model: model,
	method: "incrementalMH",
  samples: samples, burn: burn, lag: lag,
  verbose: T,
  verboseLag: totalIterations / 20
})

utils.writeQueryERP(posterior, "results/" + outfile, ["type", "param", "property", "category", "val"]);

"written to " + outfile;